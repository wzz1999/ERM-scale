using Parameters, Random, StatsBase, Printf, Distributed, LinearAlgebra, Dates
using Hungarian, Plots, Distributions
using ToeplitzMatrices
using Distances
using MultivariateStats, Statistics, Clustering, StatsPlots, Bootstrap
using LaTeXStrings
using MAT, Distributions
using LowRankApprox, MATLAB
using JSON, SHA, BSON
using InvertedIndices
using SpecialFunctions
using Roots, ForwardDiff
using QuadGK, SpecialFunctions
using MDBM
import Base.*
using CurveFit
using NearestNeighbors
using HypergeometricFunctions
using KernelDensity, Interpolations, DataInterpolations, KernelFunctions
using Flux
using Flux: @epochs
using CUDA, CUDAKernels, KernelAbstractions
using Tullio
using GaussianMixtures

fit = Distributions.fit


@with_kw mutable struct ERMParameter
    N::Int      #number of neurons
    L::Float64  #region size 2*r
    œÅ::Float64  #density
    n::Int      # dimensionality
    œµ::Float64  # parameter in distance function f
    Œº::Float64  # parameter in distance function f
    Œæ::Float64  # parameter in distance function f 
    Œ≤::Float64 = 1/Œæ # parameter in distance function f Œ≤ = 1/Œæ
    œÉÃÑ¬≤::Float64 # mean variance of neural activity
    œÉÃÑ‚Å¥::Float64  # mean 4th moment of neural activity
end
#=
@with_kw mutable struct ERMParameter
    N::Int      #number of neurons
    L::Float32  #region size 2*r
    œÅ::Float32  #density
    n::Int      # dimensionality
    œµ::Float32  # parameter in distance function f
    Œº::Float32  # parameter in distance function f
    Œæ::Float32  # parameter in distance function f 
    Œ≤::Float32 = 1/Œæ # parameter in distance function f Œ≤ = 1/Œæ
    œÉÃÑ¬≤::Float32 # mean variance of neural activity
    œÉÃÑ‚Å¥::Float32  # mean 4th moment of neural activity
end
=#
function degree_distribution(W::Array; œµ = 1f-4)

    N = size(W,1)
    A = copy(W)
    #adjacency matrix
    A[abs.(W) .< œµ] .= 0.0
    A[abs.(W) .> œµ] .= 1.0
    A = A .- diagm(ones(N))
    k_in = vec(sum(A, dims=2))
    k_out = vec(sum(A, dims=1))

    xrange_in = range(minimum(k_in),stop=maximum(k_in), length=50)
    h_in = fit(Histogram, k_in, xrange_in)
    h_in = normalize(h_in, mode=:pdf)
    x_in, p_in = dropzero(h_in) #delete bins with weights = 0

    xrange_out = range(minimum(k_out), stop=maximum(k_out), length=50)
    h_out = fit(Histogram, k_out, xrange_out)
    h_out = normalize(h_out, mode=:pdf)
    x_out, p_out = dropzero(h_out)

    plot(x_in, p_in, markershape=:circle, label = "in degree" )
    plot!(x_out, p_out, markershape=:cross, label = "out degree" )
    plot!(xlabel = "k", ylabel = "pdf", yaxis = :log)
    png("figures/degree_distribution.png")

    return k_in, k_out

end

function weight_distribution(W::Array, filename; xlimit = (1f-3, 1), xlabels = "weights", space = "linspace")

    weights = vec(W)
    weights = weights[weights.>0]
    if space == "linspace"
        w_range = range(minimum(weights), stop=maximum(weights), length=100)
    elseif space == "logspace"
        e1 = log10(minimum(weights))
        e2 = log10(maximum(weights))
        w_range = 10 .^ range(e1,e2,length = 50)
    end

    h_w = fit(Histogram, weights, w_range)
    h_w = normalize(h_w, mode=:pdf)

    x, p = dropzero(h_w)
    plot(x, p, markershape=:auto)
    plot!(xlabel = xlabels, ylabel = "pdf", xlims=xlimit, xaxis = :log, yaxis = :log)
    png("figures/"*"$filename")

    return nothing

end

function dropzero(h::Histogram)
    @unpack edges, weights = h
    binsize = edges[1][2] - edges[1][1]
    x = collect(edges[1][2:end].-binsize/2)
    #center of a bin
    idx = findall(iszero,weights)
    deleteat!(x,idx)
    deleteat!(weights,idx)
    return x, weights
end

function unpack_histogram(h::Histogram)
    @unpack edges, weights = h
    binsize = edges[1][2] - edges[1][1]
    x = collect(edges[1][2:end].-binsize/2)
    #center of a bin
    return x, weights
end

function dropzeros_and_less(y::Vector)
    x = collect(1:length(y))
    idx = findall(y.<=0)
    deleteat!(x,idx) 
    deleteat!(y,idx)
    return x, y
end


function find_hubs(y::Vector, ip; nums = 10)
    p = sortperm(y,rev=true)
    idx = p[1:nums]
    return idx, ip[idx]
end

function find_hubs(y::Vector; nums = 10)
    p = sortperm(y,rev=true)
    return p[1:nums]
end

function sample_from_powerlaw(;xmin=0.1, xmax=10, Œ± = -2.9, N = 1000, trials = 21)
    #draw random samples from a powerlaw pdf p(x) ~ x·µÖ where x ‚àà [xmin, xmax]    

    y = rand(Uniform(0, 1), N, trials)
    return ((xmax^(Œ±+1) - xmin^(Œ±+1)).*y .+ xmin^(Œ±+1)).^(1/(Œ±+1))

end

compute_P0!(x) = length(findall(iszero, x))/length(x)
#compute the probability that a neuron is not active

function set_initialcondition!(x; N = 100, trials = 21)

    P0 = compute_P0!(x)
    rnd = rand(Bernoulli(1-P0), N, trials)
    h0 = rnd.*sample_from_powerlaw(N = N, trials = trials)

    return h0

end

function normalization(x; œµ = 1.0e-6)
    a = copy(x)
    a[a.<œµ] .= 0
    for n in 1:size(a,1)
        xÃÑ = mean(a[n,a[n,:].>0])
        if (xÃÑ > 0)
            a[n,:] = a[n,:]./xÃÑ
        end
    end
    return a
end

function construct_Toeplitz(C·µ£::AbstractMatrix)

    #c = mean(sort(C·µ£, dims = 2, rev = true), dims = 1)

    iterations = 100
    N = size(C·µ£,1)
    sums = zeros(Float32,N)
    c = copy(sums)
    for n in 1:iterations
        sample!(C·µ£, c)
        sort!(c, rev = true)
        sums = sums + c
    end
    c = sums/iterations

    return convert(Matrix{Float32}, Toeplitz(c,c))

end

function construct_Toeplitz(N::Int; a = 1, b = -0.4, Œ≥ = 2)
    x = collect(1:N)
    c = a*x.^b.*exp.((x/N).^Œ≥)
    return Toeplitz(c,c)
end

function generate_heatmap(C·µ£,filename; climit = (0, 1), cbar = :thermal, xlabel = "neuron #", ylabel = "neuron #")

    gr(format=:png) #faster plotting heatmap
    theme(:dark)
    N = size(C·µ£,1)
    M = size(C·µ£,2)

    display(heatmap(C·µ£, color=cgrad(cbar),clim=climit, 
            aspect_ratio=1, xlims=(1,M), ylims=(1, N),
            xlabel = xlabel,ylabel= ylabel, title = filename))
    png("figures/"*"$filename")

end

function visualize_fr(firingrate,filename; climit = (0, 1), cbar = :ice)

    gr()
    theme(:dark)
    N = size(firingrate,1)
    M = size(firingrate,2)
    firingrate = firingrate .- mean(firingrate,dims=2)

    heatmap(firingrate, color=cgrad(cbar),clim=climit, 
            xlims=(1, M), ylims=(1, N),
            xlabel="frame #",ylabel="neuron #")
    png("$filename")

end

function diff_cost(U,H,ip,a,b,p)

    k1=ip[a]
    k2=ip[b]
    ip_new = ip[p]
    r1, r2, a1, a2 = 0, 0, 0, 0
    @inbounds @simd for i=1:length(ip)
        r1 += (H[i,a] - U[ip[i],k1])^2
        r2 += (H[i,b] - U[ip[i],k2])^2
        a1 += (H[i,a] - U[ip_new[i],k2])^2 
        a2 += (H[i,b] - U[ip_new[i],k1])^2
    end

    reduced= 2*(r1+r2) - (H[a,a] - U[k1,k1])^2 - (H[b,b]-U[k2,k2])^2
    increased= 2*(a1+a2) - (H[a,a] - U[k2,k2])^2 - (H[b,b] - U[k1,k1])^2
    dE = increased - reduced
    return dE

end


function min_fnorm(A, B, ip; MAX_STEPS=3000, t = 20.0)

    #find a permutation matrix that would minimize ||P*A*P'-B|| via simulated annealing
    #initial temperature
    Tfactor=0.999
    nover=10000
    #temperature will change to t*Tfactor after nover times trials of random swapping
    nlimit=1000
    #In one loop with nover trials, if nlimit swaps whose cost change
    #satisfies the transition probability criterion, I allow the temperature to change
    #immediately, rather than going over all the nover trials.

    n = size(A,1)
    P = I(n)[ip,:]

    Cost = (norm(P*A*P'-B))^2
    
    
    s = @sprintf "initial Frobenius distance is %.1f" Cost
    println(s)

    E = Float64[];
    T = Float64[];
    push!(E,Cost)
    push!(T,t)

    @inbounds for j in 2:MAX_STEPS
        #The maximum number of annealing steps is MAX_STEPS. After each step, the
        #temprature changes by t*Tfactor.
        nsucc, n_up, n_down = 0, 0, 0
        
        @inbounds for k in 1:nover
            (a, b) = samplepair(n)
            #randomly swap two nodes whose indexes are given by a and b in the
            #look up table iorder.
            p = collect(1:n)
            p[a] = b
            p[b] = a  
           
            dE = diff_cost(A,B,ip,a,b,p)
           
            #the mismatch cost difference  before swapping and after swapping is calculated in function edit_distance_diff.  
            #The calculation is first done for chemical synapses.
                
            answer=(dE<0)||(first(rand(1)) < exp(-dE/t))
            #if dD_E<0, or if dD_E>0 and rand(1)<exp(-dD_E/t), the swaps are allowed. 
            if answer
                nsucc=nsucc+1
                if dE < 0
                    n_down = n_down + 1
                elseif dE > 0
                    n_up = n_up + 1
                end
                Cost = Cost + dE
                ip = ip[p] 
                #The new look up table is updated.
            end
        
            if (nsucc>=nlimit)
                break
            end
            #if there are nlimit successful swaps, break the loop and change
            #the temperature.
        
        end
    
        push!(E,Cost)
        push!(T,t)
        s = @sprintf "Energy cost is E=%.1f, T=%.1f" Cost t
        println(s)
        println("Successful Moves: $nsucc")
        println("Moving up: $n_up, Moving down: $n_down")
        t=t*Tfactor;
        if (nsucc==0)
            return ip, E, T
        end
        #If  there are no successful swaps, we find the minimum.
    end

    return ip, E, T

end


function Umeyama(A,B)

    N = size(A,1)
    U‚Çê = eigvecs(A)
    U·µ¶ = eigvecs(B)
    UÃÑ‚Çê = abs.(reverse(U‚Çê,dims=2))
    UÃÑ·µ¶ = abs.(reverse(U·µ¶,dims=2))
    weight = -UÃÑ‚Çê*transpose(UÃÑ·µ¶)
    matching = Hungarian.munkres(weight)
    ip = [findfirst(matching[i,:].==Hungarian.STAR) for i = 1:N]

    return ip

end

function distance_measure(Coordinates, P)

    Coord = P*Coordinates
    R = pairwise(Euclidean(),Coord,dims=1)  

    N = size(P,1)
    r = zeros(N)
    #mean pairwise distance
    #check wether r = f(|i-j|)

    @inbounds @simd for i in 1:N
        @inbounds @simd for j in 1:N
                            idx = abs(j-i)+1
                            r[idx] += R[j,i]
        end
    end 
    
    for i in 1:N-1
        r[i+1] = r[i+1]/(2*(N-i))
    end
    r[1] = r[1]/N

    plot(collect(1:N-1), r[2:end], markershape=:auto)
    plot!(xlabel = "rank", ylabel = "distance", xaxis = :log, yaxis = :log)
    png("figures/rank-distance.png")

    R = R./mean(R[:])
    theme(:dark)
    heatmap(R, color=cgrad(:thermal),clim=(0, 2), aspect_ratio=1, xlims=(1,N), ylims=(1, N),xlabel="neuron #",ylabel="neuron #")
    png("figures/pairwise_distance.png")
    return R
end


function C_projection(r; dims=1)

    if isempty(dims) 
        return r
    else
        #### PCA #######
        M = fit(PCA, r)
        Pr = projection(M)
        #principle components
        P‚Çõ = Pr[:,Not(dims)]
        #P‚Çõ = Pr[:,1:dims] 
        #exclude the first dims component
        Y‚Çõ = transform(M,A)[Not(dims),:]
        #Y‚Çõ = transform(M,A)[1:dims,:]
        #scores

        A‚Çõ = P‚Çõ*Y‚Çõ
        #C·µ£ = A‚Çõ*A‚Çõ'/T
        return A‚Çõ # return the reconstructed activity in the subspace
    end

end

function expand(indices_bound, N)

    # (a, b, c, d) --> [a:b] \union [c:d]  

    K = length(indices_bound)
    if isodd(K)
        indices_bound = tuple(indices_bound...,N...)
        K=K+1
    end

    indices = collect(indices_bound[1]:indices_bound[2])
    
    for j=3:2:K-1
        append!(indices,collect(indices_bound[j]:indices_bound[j+1]))
    end
    
    return indices

end

function mds(Covariance_matrix::AbstractMatrix, ip::Vector, indices_bound...)
    
    N = size(Covariance_matrix,1)
    P = I(N)[ip,:]
    G = *(P,Covariance_matrix,P')
    D = gram2dmat(G)
    M = fit(MDS,D,distances=true)
    embedding = transform(M)
    indices = expand(indices_bound,N)
    return embedding[:,indices], indices

end

function mds(Coordinates::AbstractMatrix, r::AbstractMatrix, ip::Vector, indices_bound...; Œª=1, dims=2)

    (N,T) = size(r)
    P = I(N)[ip,:]
    #r = normalization(P*r,œµ = 1.0e-4)
    #r = r .- mean(r,dims=2)

    indices = expand(indices_bound,size(r,1))
    
    
    #D2 = pairwise(CorrDist(),r[indices,:], dims=1)


    # d¬≤[i,j] = œÉ¬≤[i] + œÉ¬≤[j] - 2*C[i,j]
        
    G = r[indices,:]*r[indices,:]'/T
    D2=gram2dmat(G)
    
    #

    Coord = P*Coordinates
    D1 = pairwise(Euclidean(),Coord[indices,:],dims=1)  
    D1 = D1./mean(D1[:])
    D = D2 + Œª*D1
    embedding=classical_mds(D, dims)

    return embedding, D, indices

end



function plot_coordinates(Coordinates,indices,filename;Œª = 0)

    gr()
    theme(:dark)

    dims = size(Coordinates,1)

    if dims>3
        throw("cannot plot figures that are more than three dimensions!")
    end

    Coordinates = Coordinates .- mean(Coordinates,dims=2)

    if dims==3

        x = Coordinates[1,:]
        y = Coordinates[2,:]
        z = Coordinates[3,:]
        l = @layout [a b;c d]

        p1 = scatter(x,y,zcolor=indices, color=cgrad(:rainbow), aspect_ratio = 1, 
                title="x-y", markersize = 2, markerstrokewidth = 0, cbar = false, legend = false)
        p2 = scatter(z,y,zcolor=indices, color=cgrad(:rainbow), aspect_ratio = 1, 
                title="z-y", markersize = 2, markerstrokewidth = 0, cbar = false, legend = false)
        p3 = scatter(x,z,zcolor=indices, color=cgrad(:rainbow), aspect_ratio = 1, 
                title="x-z", markersize = 2, markerstrokewidth = 0, cbar = false, legend = false)
        p4 = scatter(x,y,z, zcolor=indices, color=cgrad(:rainbow), markersize = 2, 
                markerstrokewidth = 0, cbar = true, w = 0.5, legend = false, 
                title=L"\lambda = %$Œª, d = %$dims")
        plot(p1,p2,p3,p4,layout=l)

        savefig("figures/"*"$filename"*".pdf")

    elseif dims==2

        scatter(Coordinates[1,:],Coordinates[2,:],zcolor=indices, 
                color=cgrad(:rainbow), aspect_ratio = 1, markerstrokewidth = 0, legend = false, cbar = true, 
                title=L"\lambda = %$Œª, d = %$dims")
        png("figures/"*"$filename"*".png")
        
    elseif dims==1

        scatter(Coordinates[1,:],zcolor=indices, 
                color=cgrad(:rainbow), aspect_ratio = 1, markerstrokewidth = 0, legend = false, cbar = true, 
                title=L"\lambda = %$Œª, d = %$dims")
        png("figures/"*"$filename"*".png")
    end

end

function *(A::T,B::T,C::T) where T<:Array{Real}

    (N,M) = size(A)
    if N!=M
        throw("should be a square matrix!")
    end

    (N,M) = size(B)
    if N!=M
        throw("should be a square matrix!")
    end

    (N,M) = size(C)
    if N!=M
        throw("should be a square matrix!")
    end

    temp = zeros(T,(N,N))
    D = zeros(T,(N,N))
    mul!(temp,A,B)
    mul!(D,temp,C)
    return D
end
    


function finite_sampling(r,ip)
# create a synthetic covariance matrix based on the principle of finite_sampling
    (N,T) = size(r)
    P = I(N)[ip,:]
    r = normalization(P*r,œµ = 1.0e-4)
    r = r .- mean(r,dims=2)
    C = zeros(Float32,N,N)
    mul!(C,r,r') #experimental matrix
    C = C/T
    C‚Çõ = construct_Toeplitz(C)
    
    G = randn(Float32,(N,N))/sqrt(N)
    C·µ£ = *(G,C‚Çõ,G') #random matrix based on finite sampling
    
    Œ± = tr(C)/tr(C·µ£) #scaling factor
    C·µ£ = Œ±*C·µ£

    U = eigvecs(C·µ£)
    V = eigvecs(C)

    U = convert(Array{Float32},U)
    V = convert(Array{Float32},V)

    Pr = zeros(Float32,N,N)
    mul!(Pr,V,U')
    return *(Pr,C·µ£,Pr')

end

function block_sampling(C::AbstractMatrix,K::Int; p = 0.1)
    N = size(C,1)
    S = randsubseq(1:N-K-1,p)
    n = length(S)
    Œõ = zeros(K,n)
    for i = 1:length(S)
        C_s = C[S[i]:S[i]+K-1,S[i]:S[i]+K-1]
        Œª = eigvals(C_s)
	    sort!(Œª, rev=true)
        Œõ[:,i] = Œª
    end
    return mean(Œõ,dims=2)
end


function data_xy(datafile::String; seqlen = 100, xsize = 1, nullinputsize = 1, trials = 20)
    @unpack pre_converging_frames, onset_frames, FiringRate, predict_eye_conv_ROI_id, neural_neural_consistency_metric, ROI = matread(datafile)

    N = size(FiringRate,1)
    inputsize = length(predict_eye_conv_ROI_id)
    
    if trials > length(pre_converging_frames)
        trials = length(pre_converging_frames)
        println("trial number is bounded by the data, and is now set to $trials")
    end
    
    input_idx = Int.(predict_eye_conv_ROI_id)
    nullinput_idx = setdiff(shuffle(1:N), input_idx)
    neuron_idx = cat(input_idx, nullinput_idx, dims = 1)[1:inputsize+nullinputsize]

    ysize = inputsize+nullinputsize

    y = zeros(ysize, seqlen, trials)

    if xsize < ysize 
        xsize = ysize
    end

    x = zeros(xsize, seqlen, trials)
    onset_frames = Int.(onset_frames)
    pre_converging_frames = Int.(pre_converging_frames)
    w = 1
    
    for n = 1:trials
        start_frame = pre_converging_frames[n]
        end_frame = start_frame + seqlen - 1
        frames = start_frame:end_frame
        y[:,:,n] = FiringRate[neuron_idx,frames]
        for k = 1:length(onset_frames) #it is possible that in each sequence, there are a few events
            if onset_frames[k] in frames
                stimulated_frames = onset_frames[k] - start_frame - 5 : onset_frames[k] - start_frame - 3
                x[1:inputsize,stimulated_frames,n] .= w*neural_neural_consistency_metric[:,k] #present external inputs
            end
        end
    end
    
    raw_neuron_idx = ROI[neuron_idx]

    return x, y, raw_neuron_idx
end

function data_xy(firingrate::T; seqlen = 1000, ysize = 1) where T<:Array{Float32,2}

    N = size(firingrate,1)
    if ysize < N
        neuron_idx = shuffle(1:N)[1:ysize]
        r_train = firingrate[neuron_idx,1:seqlen]
    elseif ysize == N
        r_train = firingrate[:,1:seqlen]
        neuron_idx = collect(1:N)
    else
        error("ysize is larger than the number of neurons in the dataset!")
        return nothing
    end

    return r_train, neuron_idx
    
end

function data_xy(firingrate::T, sequence::UnitRange{Int64}; ysize = 1) where T<:Array{Float32,2} 

    N = size(firingrate,1)
    if ysize<N
        neuron_idx = shuffle(1:N)[1:ysize] 
        r_train = firingrate[neuron_idx,sequence]
    elseif ysize == N
        r_train = firingrate[:,sequence]
        neuron_idx = collect(1:N)
    else
        error("ysize is larger than the number of neurons in the dataset!")
        return nothing
    end

    return r_train, neuron_idx
    
end

function predict_CovarianceMatrix(J,C·µ£; stepsize = 0.1)

    a = collect(0.1:stepsize:1)
    n = length(a)
    cost = zeros(n)

    for i in 1:n
        C = predict_CovarianceMatrix(J,C·µ£,a[i])
        cost[i] = norm(C-C·µ£)
    end

    (value,index) = findmin(cost)
    return value, a[index]
end

function predict_CovarianceMatrix(J,C·µ£,a)
    N = size(C·µ£,1)
    K = inv(I(N)-a*J)
    C = K*K'
    Œ± = tr(C·µ£)/tr(C) #scaling factor
    C = Œ±*C
    return C
end

function lowrank_approx(J; rank = 200)
    mat"""
        [$U,$S,$V] = svdsketch($J, 4e-4, 'MaxSubspaceDimension',$rank);
    """
    return U*S*V'
end

function RG_analysis(spike,root)
    mat"""
        matlabroot = getenv('MATLAB_PROJECT');
        addpath([matlabroot, '/code/RG/functions']);
        N = size($spike,1);
        iterations = floor(log2(N));
        h=coarse_grain($spike,iterations,"dims",1,"do_toeplitz_construction",1,'NORM',0);
        saveas(h,[$root,'/eigen.png']);
        close all;
    """
end


function gendir(param,root)
    dir = mkpath(joinpath(root,bytes2hex(sha1(json(param))))) 
    open(joinpath(dir, "param.json"), "w") do io 
    JSON.print(io, param, 4) 
    end 
    return dir 
end 

function binary_connectivity(J;threshold = 0.1)
    N = size(J,1)
    J_t = zeros(eltype(J),N,N)
    J_t .= J
    J_t[abs.(J).<threshold].= 0.0
    J_t[J.>threshold] .= 1.0
    J_t[J.<-threshold] .= -1.0
    return J_t
end

function randomclusters(N)
    mat"""
        matlabroot = getenv('MATLAB_PROJECT');
        addpath([matlabroot, '/code/RG/functions']);
        $neural_set = random_clusters($N);
    """
    return neural_set
end

function similarity_matching(a;trials=10)
    gr(format=:png)
    (N,T) = size(a)
    k_max = convert(Int,floor(log2(N)))
    k_min = 6
    p = plot()
    for n in k_min:k_max
        K = 2^n
        G = zeros(T,T)
        for j in 1:trials 
            id = shuffle(Vector(1:N))[1:K]
            r = a[id,:]
            G = G .+ r'*r/K
        end
        G = G/trials
        Œª = eigvals(G)
        sort!(Œª, rev=true)
        rank = collect(1:K)
        plot!(p, rank, Œª[1:K],markershape=:auto, markersize=1,xaxis = :log, yaxis = :log)
        generate_heatmap(G, "gram_map"*"K=$K", climit = (0,0.01), xlabel = "time", ylabel = "time")
    end
    display(p)
end

function remove_outliers(v)
    mat"""
        $u = rmoutliers($v);
    """
    return u
end

function subsampling(C,root)
    mat"""
        matlabroot = getenv('MATLAB_PROJECT');
        addpath([matlabroot, '/code/RG/functions']);
        [h1,h2] = subsampling($C);
        saveas(h1,[$root,'/eigen.png']);
        saveas(h2,[$root,'/density.png']);
        close all;
    """
end
#add theoretical calculation to the subsampling as well
function subsampling(C::Matrix, Œª::Vector, P::Vector, p::ERMParameter, root)
    @unpack L, œÅ, Œº, œµ, n, Œæ, œÉÃÑ‚Å¥ = p
    titles = "L = $(round(L,digits=1)), \\mu = $Œº, \\epsilon = $(round(œµ,digits=3)), \\rho^{-1/d} = $(round(œÅ^(-1/n),digits=3)),
                d =  $n, \\xi = $(round(Œæ,digits=1)), \\beta = 1/\\xi, E(\\sigma^4) = $(round(œÉÃÑ‚Å¥,digits=2))"
    mat"""
        matlabroot = getenv('MATLAB_PROJECT');
        addpath([matlabroot, '/code/RG/functions']);
        [h1,h2] = subsampling($C);
        ax2 = h2.CurrentAxes;
        ax1 = h1.CurrentAxes;
        lgd = ax2.Legend.String;
        lgd{end+1}='theory';
        plot(ax2,$Œª,$P,'color',[0.5,0.5,0.5],'linewidth',2);
        legend(ax2,lgd);
        title(ax2,$titles);
        title(ax1,$titles);
        saveas(h1,[$root,'/eigenERM.png']);
        saveas(h2,[$root,'/densityERM.png']);
        close all;
    """
end

function subsampling_matrix_ensemble(Œª::Vector, P::Vector, p::ERMParameter; root = joinpath(ENV["JULIA_PROJECT"],"figures"), N = 10)
    
    @unpack L, œÅ, Œº, œµ, n, Œ≤, œÉÃÑ‚Å¥ = p
    titles = "L = $(round(L,digits=1)), \\mu = $Œº, \\epsilon = $(round(œµ,digits=3)), \\rho^{-1/d} = $(round(œÅ^(-1/n),digits=3)),
                d =  $n, \\beta = $Œ≤, E(\\sigma^4) = $(round(œÉÃÑ‚Å¥,digits=2))"
    C_ensemble = generate_ERM_ensemble(p, ensemble_size = N)
    
    mat"""
        matlabroot = getenv('MATLAB_PROJECT');
        addpath([matlabroot, '/code/RG/functions']);
        [h1,h2] = subsampling_matrix_ensemble($C_ensemble);
        ax2 = h2.CurrentAxes;
        ax1 = h1.CurrentAxes;
        lgd = ax2.Legend.String;
        lgd{end+1}='theory';
        plot(ax2,$Œª,$P,'color',[0.5,0.5,0.5],'linewidth',2);
        legend(ax2,lgd);
        title(ax2,$titles);
        title(ax1,$titles);
        saveas(h1,[$root,'/eigenERM.png']);
        saveas(h2,[$root,'/densityERM.png']);
        close all;
    """

end

function generate_ERM_ensemble(p::ERMParameter; ensemble_size = 100, different_neural_variability = true)
    
    @unpack N, L, n = p
    C_ensemble = zeros(N,N,ensemble_size)
    
    if different_neural_variability
        œÉ¬≤ = rand(LogNormal(0,0.5),N,1)
    else
        œÉ¬≤ = ones(N,1)
    end
    
    œÉ = vec(broadcast(‚àö,œÉ¬≤))
    Œî = diagm(œÉ)

    points = rand(Uniform(-L/2,L/2),N,n) #points are uniformly distributed in a region 
    for i in 1:ensemble_size
        C = reconstruct_covariance(points, p, subsample = false)
        C_ensemble[:,:,i] = Œî*C*Œî
    end
    
    return C_ensemble

end


ranking(v) = collect(1:length(v))/length(v)

function compare_v(v1::Vector,v2::Vector)
    V = hcat(v2, -v2, reverse(v2), -reverse(v2))
    D = sum(abs2, V.-v1, dims = 1)
    (d,index) = findmin(vec(D))
    return V[:,index]
end

function compare_v(V1::AbstractMatrix, V2::AbstractMatrix)
    (rows,cols) = size(V1)
    U = copy(V2)
    for i in 1:cols
        U[:,i] = compare_v(vec(V1[:,i]), vec(V2[:,i]))
    end
    return U
end

function find_eigenvectors(C)
    N = size(C,1)
    F = eigen(C)
    Œª, U = F
    U = U[:,sortperm(Œª,rev=true)]
    U = U*‚àöN
    sort!(Œª,rev=true)
    return Œª, U
end

# single thread
function random_sampling_covariance(Covariance::Matrix, U::Matrix; K = 1024, dims = 4)

    N = size(Covariance,1)
    samples = Int(round(N/K))
    W = zeros(K,dims)
    V_template = copy(W)

    for trial in 1:samples
        id = shuffle(1:N)[1:K]
        C = Covariance[id,id]
        C‚Çú = construct_Toeplitz(C)
        ip_K, E, t = min_fnorm(C, C‚Çú, collect(1:K), t=5.0, MAX_STEPS=10000)
        Pm = I(K)[ip_K,:]
        C = Pm*C*Pm'
        V = find_eigenvectors(C)
        if trial == 1
            V_template .= U[sort!(id),:]
        else
            V_template .= W/trial
        end
        W = W .+ compare_v(V_template,V[:,1:dims])
    end
    return W/samples
end

#multi-thread
function random_sampling_covariance(Covariance::Matrix, U::Matrix; K = 1024, dims = 4, samples = 10)

    N = size(Covariance,1)
    W = zeros(K,dims,samples)
    V_template = zeros(K,dims)

    Threads.@threads for trial in 1:samples
        id = shuffle(1:N)[1:K]
        C = Covariance[id,id]
        C‚Çú = construct_Toeplitz(C)
        ip_K, E, t = min_fnorm(C, C‚Çú, collect(1:K), t=5.0, MAX_STEPS=10000)
        Pm = I(K)[ip_K,:]
        C = Pm*C*Pm'
        V = find_eigenvectors(C)
        if trial == 1
            V_template .= U[sort!(id),:]
            W[:,:,1] .= compare_v(V_template,V[:,1:dims])
        else
            W[:,:,trial] .= V[:,1:dims]
        end
    end

    for trial in 2:samples
        V_template = dropdims(sum(W[:,:,1:trial-1],dims=3)/(trial-1), dims=3)
        W[:,:,trial] = compare_v(V_template,W[:,:,trial])
    end
    
    return dropdims(sum(W,dims=3)/samples, dims=3)

end


function random_sampling_similarity(a::Matrix, template::Matrix; K = 1024,  dims = 4, samples = 1)
    #a activity matrix
    #template: dims √ó T orthogonal matrix
    
    (N, T) = size(a)
    W = zeros(T,dims,samples)

    Threads.@threads for trial in 1:samples
        id = shuffle(1:N)[1:K]
        a_sampled = a[id,:]
        F = svd(a_sampled)
        Vt = F.Vt
        V = Vt'
        W[:,:,trial] .= compare_v(template,V[:,1:dims])
    end
    
    return dropdims(sum(W,dims=3)/samples, dims=3)
end


function changing_basis(C::Matrix; basis="Cauchy")
    N = size(C,1)
    if basis == "Cauchy"
        H = rand(Cauchy(0,1), N, N)
    elseif basis == "Gaussian"
        H = rand(Gaussian(0,1), N, N)
    end
    F = qr(H)
    Q, R = F

    G = eigen(C)
    Œª, U = G
    return Q*diagm(0=>vec(Œª))*Q'
end

#uppertriangular matrix excluding the diagonal
function uppertriangular(C::Matrix)
    N = size(C,1)
    m = triu(trues(N,N),1)
    return C[m]
end
#construct uppertriangular matrix from a vector
function construct_uppertriangular(C::Matrix, v::Vector)
    n = size(C,1)
    k=0
    return diagm(diag(C)) .+ [ j<i ? (k+=1; v[k]) : 0 for i=1:n, j=1:n ]'
end

function ball_point_picking(N; R = 1, dims = 3)
    points = zeros(N,dims)
    for j in 1:N
        while true
            test_point = rand(Uniform(-R,R),1,dims) 
            if norm(test_point) <= R
                points[j,:] = test_point
                break
            end
        end
    end
    return points
end

function sphere_point_picking(N; R = 1, dims = 3)
    points = zeros(N, dims)
    for j in 1:N 
        while true
            x‚ÇÅ = rand(Uniform(-R,R))
            x‚ÇÇ = rand(Uniform(-R,R))
            if ‚àö(x‚ÇÅ^2 + x‚ÇÇ^2) <= R
                x = 2*x‚ÇÅ*‚àö(1-x‚ÇÅ^2 - x‚ÇÇ^2)
                y = 2*x‚ÇÇ*‚àö(1-x‚ÇÅ^2 - x‚ÇÇ^2)
                z = 1 - 2*(x‚ÇÅ^2 + x‚ÇÇ^2)
                points[j,:] = [x y z]
                break
            end
        end
    end
    return points
end

#points picking with short-range repulsion 
function point_picking(N, d::Distribution; R = 1, œµ = 0.01, dims = 2)
    points = zeros(N,dims)
    points[1,:] = rand(d,1,dims)
    for j in 2:N
        while true
            test_point = rand(d,1,dims)
            pairwise_distance = map(norm, eachrow(points[1:j-1,:] .- test_point))
            if isempty(findall(d->d<œµ,pairwise_distance))
                points[j,:] = test_point
                break
            end
        end
    end
    return points
end




function reconstruct_covariance(points, p::ERMParameter; subsample = false)
    
    @unpack Œº, œµ, Œ≤, L = p 

    N = size(points, 1)
    D = pairwise(Euclidean(), points, dims=1)
    
    C = copy(D)
    #C[D .<= œµ] .= 1
    #C[D .> œµ] .= œµ^Œº*(D[D .> œµ]).^(-Œº).*exp.(-(D[D .> œµ] .- œµ)*Œ≤)
    f(d) = œµ^Œº*(œµ^2 .+ d^2).^(-Œº/2)
    C .= œµ^Œº*(œµ^2 .+ D.^2).^(-Œº/2)
    #C[D .< L] .= f.(D[D .< L])
    #C[D .> L] .= f.(L * exp.(D[D .> L]./L .- 1))
    #D = D .+ Œ±*ones(N,N)
    #D = D .+ diagm(ones(N))
    #C = D.^Œ≤
    #C = D.^Œ≤.*exp.(-D/Œæ)
    #C = exp.(-D.^2/Œæ)
    #Œ≥ = N/tr(C)
    #C = C*Œ≥
    if subsample
        subsampling(C, "./figures")
    end

    return C
end

function reconstruct_covariance(points, p::ERMParameter, f_r; subsample = false)
    
    @unpack Œº, œµ, Œ≤ = p 

    N = size(points, 1)
    D = pairwise(Euclidean(), points, dims=1)
    
    C = copy(D)
    C = f_r(C,p)
    if subsample
        subsampling(C, "./figures")
    end

    return C
end

function ERM_eig_trial(p::ERMParameter;ntrial=3, points0 = nothing)
    @unpack L,N,n = p
    d = n
    eig_trial = zeros((N,ntrial))
    Threads.@threads for t = 1:ntrial
        if points0 == nothing
            points = rand(Uniform(-L/2,L/2),N,d)
        else
            points = points0
        end
        C = reconstruct_covariance(points, p)
        Œª = sort(eigvals(Hermitian(C)), rev=true)
        eig_trial[:,t] = Œª
    end
    return eig_trial
end


function eigendensity(C::Matrix, p::ERMParameter; length = 50, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10^10)

    Œª = eigvals(C)
    id, Œª = dropzeros_and_less(Œª)
    N = size(C,1)

    #Œª·µ£ = range(minimum(Œª),maximum(Œª),length=length)

    h_Œª = fit(Histogram, Œª, LogRange(minimum(Œª),maximum(Œª),length))
    h_Œª = normalize(h_Œª, mode=:pdf)

    Œª·µ£, p_sim = dropzero(h_Œª)
    p_sim = p_sim.*size(id,1)/N #correction for deleting negative eigenvalues 

    @unpack œÉÃÑ¬≤ = p

    if use_variation_model
        œÉ¬≤ = diag(C)
        Œª_theory, p_theory = eigendensity_variation_model(Œª·µ£,p_sim,œÉ¬≤,p,Œª_min = Œª_min,Œª_max = Œª_max)
        id, p_theory = dropzeros_and_less(p_theory.-10^-6)
        Œª_theory = Œª_theory[id]
        p_theory = p_theory .+ 10^-6
    else
        if !correction
            Œª_theory, p_theory = eigendensity(Œª·µ£/œÉÃÑ¬≤,p, Œª_min = Œª_min)
            p_theory = p_theory/œÉÃÑ¬≤
        else
            Œª_theory, p_theory = eigendensity_with_correction(Œª·µ£,p, Œª_min = Œª_min)
        end
    end

    return Œª·µ£, p_sim, Œª_theory, p_theory

end

function eigendensity(C::Matrix; length = 50, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10^10)

    Œª = eigvals(C)
    id, Œª = dropzeros_and_less(Œª)
    N = size(C,1)

    #Œª·µ£ = range(minimum(Œª),maximum(Œª),length=length)

    h_Œª = fit(Histogram, Œª, LogRange(minimum(Œª),maximum(Œª),length))
    h_Œª = normalize(h_Œª, mode=:pdf)

    Œª·µ£, p_sim = dropzero(h_Œª)
    p_sim = p_sim.*size(id,1)/N #correction for deleting negative eigenvalues 

    return Œª·µ£, p_sim

end

function eigendensity_range(C::Matrix; length = 10, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10)

    Œª = eigvals(C)
    id, Œª = dropzeros_and_less(Œª)
    N = size(C,1)

    #Œª·µ£ = range(minimum(Œª),maximum(Œª),length=length)
    N2 = size(findall(Œª_min.< Œª .<Œª_max))[1]
    h_Œª = fit(Histogram, Œª, LogRange(Œª_min, Œª_max,length))
    h_Œª = normalize(h_Œª, mode=:pdf)

    Œª·µ£, p_sim = h_Œª.edges , h_Œª.weights
    p_sim = p_sim.*N2/N #correction for deleting negative eigenvalues 

    binsize = Œª·µ£[1][2] - Œª·µ£[1][1]
    Œª·µ£ = collect(Œª·µ£[1][2:end].-binsize/2)
    return Œª·µ£, p_sim

end

function eigendensity(p::ERMParameter; length = 100, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10^10, m = 100, different_neural_variability = false, theory = true, œÉ¬≤ = Nothing)

    @unpack N, L, n = p
    Œª_all = zeros(m,p.N)
    for i = 1:m
        points = rand(Uniform(-L/2,L/2),N,n)
        C = reconstruct_covariance(points, p, subsample = false)
        if different_neural_variability
            if œÉ¬≤ == Nothing
                œÉ¬≤ = vec(rand(LogNormal(0,0.5),N,1))
            end
        else
            œÉ¬≤ = ones(N,1)
        end
        œÉ = vec(broadcast(‚àö,œÉ¬≤))
        Œî = diagm(œÉ)
        CÃÇ = Œî*C*Œî
        Œª = eigvals(CÃÇ)
        Œª_all[i,:] = Œª
    end

    Œª = reshape(Œª_all,p.N*m)
    Œª = sort(Œª, rev=true)
    id, Œª = dropzeros_and_less(Œª)

    #Œª·µ£ = range(minimum(Œª),maximum(Œª),length=length)

    h_Œª = fit(Histogram, Œª, LogRange(minimum(Œª),maximum(Œª),length))
    h_Œª = normalize(h_Œª, mode=:pdf)

    Œª·µ£, p_sim = dropzero(h_Œª)
    p_sim = p_sim.*size(id,1)/m/N #correction for deleting negative eigenvalues 

    @unpack œÉÃÑ¬≤ = p

    if theory
        if use_variation_model
            œÉ¬≤ = diag(C)
            Œª_theory, p_theory = eigendensity_variation_model(Œª·µ£,p_sim,œÉ¬≤,p,Œª_min = Œª_min,Œª_max = Œª_max)
            id, p_theory = dropzeros_and_less(p_theory.-10^-6)
            Œª_theory = Œª_theory[id]
            p_theory = p_theory .+ 10^-6
        else
            if !correction
                Œª_theory, p_theory = eigendensity(Œª·µ£/œÉÃÑ¬≤,p, Œª_min = Œª_min)
                p_theory = p_theory/œÉÃÑ¬≤
            else
                Œª_theory, p_theory = eigendensity_with_correction(Œª·µ£,p, Œª_min = Œª_min)
            end
        end

        return Œª·µ£, p_sim, Œª_theory, p_theory, [Œª[Int(N*m/10)], Œª[Int(N*m/100)]]
    end

    if !theory
        return Œª·µ£, p_sim, [Œª[Int(N*m/10)], Œª[Int(N*m/100)]]
    end

end

#########  compute cov pdf by corr pdf ####################

function eigendensity_convolution(p::ERMParameter; length = 100, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10^10, m = 100, different_neural_variability = false, theory = true, œÉ¬≤ = Nothing)

    @unpack N, L, n = p
    Œª_all = zeros(m,p.N)
    Œª_all2 = zeros(m,p.N)
    for i = 1:m
        points = rand(Uniform(-L/2,L/2),N,n)
        C = reconstruct_covariance(points, p, subsample = false)
        if different_neural_variability
            if œÉ¬≤ == Nothing
                œÉ¬≤ = vec(rand(LogNormal(0,0.5),N,1))
            end
        else
            œÉ¬≤ = ones(N,1)
        end
        œÉ = vec(broadcast(‚àö,œÉ¬≤))
        Œî = diagm(œÉ)
        CÃÇ = Œî*C*Œî
        Œª = eigvals(CÃÇ)
        Œª2 = eigvals(C) + vec(rand(LogNormal(0,0.5),N,1).^2) .-1
        Œª_all[i,:] = Œª
        Œª_all2[i,:] = Œª2
    end

    Œª = reshape(Œª_all,p.N*m)
    Œª = sort(Œª, rev=true)
    id, Œª = dropzeros_and_less(Œª)

    #Œª·µ£ = range(minimum(Œª),maximum(Œª),length=length)

    h_Œª = fit(Histogram, Œª, LogRange(minimum(Œª),maximum(Œª),length))
    h_Œª = normalize(h_Œª, mode=:pdf)

    Œª·µ£, p_sim = dropzero(h_Œª)
    p_sim = p_sim.*size(id,1)/m/N #correction for deleting negative eigenvalues 
    #############################################################
    Œª2 = reshape(Œª_all2,p.N*m)
    Œª2 = sort(Œª2, rev=true)
    id, Œª2 = dropzeros_and_less(Œª2)

    #Œª·µ£ = range(minimum(Œª),maximum(Œª),length=length)

    h_Œª2 = fit(Histogram, Œª2, LogRange(minimum(Œª),maximum(Œª),length))
    h_Œª2 = normalize(h_Œª2, mode=:pdf)

    Œª·µ£2, p_sim2 = dropzero(h_Œª2)
    p_sim2 = p_sim2.*size(id,1)/m/N #correction for deleting negative eigenvalues 
    
    return Œª·µ£, p_sim, Œª·µ£2, p_sim2

end


#########  high density theory  ####################

function eigendensity(Œõ::Vector, p::ERMParameter; Œª_min = 1)

    @unpack œÅ, Œº, œµ, n, Œ≤ = p

    S = 2œÄ^(n/2)/gamma(n/2)
    large_eigs = findall(x->x>=Œª_min, Œõ)
    ùú¶ = Œõ[large_eigs]
    k0 = guess_zeros(ùú¶/œÅ,p)
    Prob = similar(ùú¶)

    for j in 1:length(ùú¶)

        Œª = ùú¶[j]
        k = fÃÇ_inv(Œª/œÅ,p,k0[j])
        Prob[j] = S/((2œÄ)^n) * k^(n-1)/(œÅ^2*abs(‚àÇfÃÇ(k,p)))

    end

    return ùú¶, Prob

end

####### high density theory with second order correction #############

function eigendensity_with_correction(Œõ::Vector, p::ERMParameter; Œª_min = 1)


    @unpack œÅ, Œº, œµ, n, Œ≤, L, œÉÃÑ¬≤, œÉÃÑ‚Å¥ = p

    S = 2œÄ^(n/2)/gamma(n/2)

    #determine upper and lower cutoff wave vector
    #upper cutoff wave vector is proportional to œÅ^(1/n): 
    #where a is the lattice size œÅ^(-1/n)
    #lower cutoff is given by the box diameter L 
    #kx ‚àà [-œÄ/a, -œÄ/L]&&[œÄ/L, œÄ/a], ky ‚àà [-œÄ/a, -œÄ/L]&&[œÄ/L, œÄ/a]
    # |k| = ‚àö(kx¬≤ + ky¬≤)
    
    #k‚ÇÇ = œÄ*œÅ^(1/n)
    k‚ÇÇ = œÄ/œµ
    k‚ÇÅ = œÄ/L

    ùùÜ = œÅ*œÉÃÑ¬≤ #simplify the notations below 

    
    large_eigs = findall(x -> x>=Œª_min, Œõ)
    ùú¶ = Œõ[large_eigs]
    k0 = guess_zeros(ùú¶/ùùÜ,p)
    Prob = similar(ùú¶)

    for j in 1:length(ùú¶)

        Œª = ùú¶[j]
        k = fÃÇ_inv(Œª/ùùÜ,p,k0[j])
        P‚ÇÄ = S/((2œÄ)^n) * k^(n-1)/(œÅ * ùùÜ * abs(‚àÇfÃÇ(k,p)))
        P‚ÇÅ = 1/2 * (S/((2œÄ)^n))^2 * k^(n-1)/(ùùÜ^2 * abs(‚àÇfÃÇ(k,p))) * œÉÃÑ‚Å¥/ùùÜ * 
        (-g(k‚ÇÇ,p)*Œª/(Œª - ùùÜ*fÃÇ(k‚ÇÇ,p))+
        g(k‚ÇÅ,p)*Œª/(Œª - ùùÜ*fÃÇ(k‚ÇÅ,p))+
        quadgk_cauchy(x->Q1(x,p,Œª),fÃÇ(k‚ÇÇ,p), Œª/ùùÜ, fÃÇ(k‚ÇÅ,p)))
        P‚ÇÇ = 1/2 * (S/((2œÄ)^n))^2 * œÉÃÑ‚Å¥/(ùùÜ^2*abs(‚àÇfÃÇ(k,p)))*
                ‚àÇg(k,p) * quadgk_cauchy(x->Q2(x,p),fÃÇ(k‚ÇÇ,p), Œª/ùùÜ, fÃÇ(k‚ÇÅ,p))
        Prob[j] = P‚ÇÄ - P‚ÇÅ - P‚ÇÇ
    end

    return ùú¶, Prob 

end

function f_r_t_family(r, p::ERMParameter)
    @unpack Œº, œµ, Œ≤ = p
    c = (1/œµ^Œº)^(-2/Œº)
    y = œµ^Œº.*(c .+ r.^2).^(-Œº/2)
    return y
end

f_r = f_r_t_family

#######  Fourier transform of distance function and its various corrections ##############
include("t_pdf_FT.jl")

#fÃÇ(x, p) = fÃÇ‚ÇÄ(x, p) + fÃÇ_correction(x,p)
#‚àÇfÃÇ‚ÇÄ(x,p) = ForwardDiff.derivative(x -> fÃÇ‚ÇÄ(x,p),x)
#‚àÇfÃÇ(x,p) = ‚àÇfÃÇ‚ÇÄ(x,p) + ‚àÇfÃÇ_correction(x,p)

#fÃÇ(x, p) = fÃÇ‚ÇÄ_num(x,p)

fÃÇ(x, p) = fÃÇ‚ÇÄ_th(x, p)
‚àÇfÃÇ(x,p) = ForwardDiff.derivative(x -> fÃÇ(x,p),x)

#‚àÇ¬≤fÃÇ(x,p) = ForwardDiff.derivative(x -> ‚àÇfÃÇ(x,p),x)
#fÃÇ(x, p) = min(fÃÇ‚ÇÄ(x, p) + fÃÇ_correction(x,p), fÃÇ_numerical(10^-8, p))
#fÃÇ(x, p) = fÃÇ_numerical(x, p)


#‚àÇ¬≤fÃÇ(x,p) = ‚àÇ¬≤fÃÇ‚ÇÄ(x,p) + ‚àÇ¬≤fÃÇ_correction(x,p)

#‚àÇ¬≤fÃÇ‚ÇÄ(x,p) = ForwardDiff.derivative(x -> ‚àÇfÃÇ‚ÇÄ(x,p),x)

function fÃÇ‚ÇÄ(x, p)

    @unpack Œº, Œ≤, œµ, n = p

    if (Œ≤!=0) && (Œº!=0)
        Œ±‚ÇÅ = Œº*(1 - Œº*log(2))/(1 - Œº)
        Œ±‚ÇÇ = (1-2*Œº)*(1+Œº*log(2))*Œ≤/(1-Œº)
        k = Œ≤/x * (Œ≤^2/x^2 + 1)^(-1/2)
        #return exp(Œ≤*œµ)*2œÄ*œµ^Œº/x^(2-Œº)*(Œ±‚ÇÇ/x + Œ±‚ÇÅ)*gamma(2 - Œº)
        return exp(Œ≤*œµ)*2œÄ*œµ^Œº/x^(2-Œº)*gamma(2 - Œº)*pFq([-(1-Œº),(1-Œº)+1], [1],(1-k)/2)
        #special case only for n=2, othercase has not been computed 
    elseif Œ≤==0
        c = œÄ^(n/2)*2^Œº*gamma(Œº/2)/gamma((n-Œº)/2)
        return exp(Œ≤*œµ)*(2œÄ)^n*œµ^Œº/c/x^(n-Œº)
        #https://en.wikipedia.org/wiki/Fourier_transform, power law function
    elseif Œº==0
        c = gamma((n+1)/2)/œÄ^((n+1)/2)
        return exp(Œ≤*œµ)*c*Œ≤*exp(Œ≤*œµ)*(2œÄ)^n/((Œ≤^2 + x^2)^((n+1)/2))
        #https://en.wikipedia.org/wiki/Fourier_transform, exponential function
    end
end

function fÃÇ_numerical(k, p)
    @unpack Œº, Œ≤, œµ, n, L = p
    return ùîÖ(k,p) + quadgk(x -> ùîâ(x,(k,p)), k*œµ, k*L)[1]
end

function fÃÇ_correction(k,p)
    @unpack Œº, œµ, Œ≤ = p
    #return -Œº*œÄ*œµ^2/(2-Œº)
     if Œº!=0
        return ùîÖ(k,p) - quadgk(x -> ùîâ(x,(k,p)), 0, k*œµ)[1]
        #return 0
     else
        return 0
     end
end

function ùîÖ(k,p)
    @unpack Œº, œµ, n = p
    return (2œÄ*œµ/k)^(n/2)*besselj(n/2,k*œµ)
end

function ùîâ(x,param)
    k, p = param
    @unpack Œ≤, Œº, œµ, n = p
    #return (2œÄ)^(n/2)*œµ^Œº/k^(n-Œº)*exp(-Œ≤*x/k)*x^(n/2-Œº)*besselj(n/2-1,x) #œµ ‚â™ 1/Œ≤
    return exp(Œ≤*œµ)*(2œÄ)^(n/2)*œµ^Œº/k^(n-Œº)*x^(n/2-Œº)*besselj(n/2-1,x)
end

function ùî£(k,p) 
    # ùî£(k) = ùîâ(kœµ)
    @unpack Œº, œµ, n, Œ≤ = p
    return exp(Œ≤*œµ)*(2œÄ)^(n/2)*œµ^Œº/k^(n/2)*œµ^(n/2-Œº)*besselj(n/2-1,k*œµ)
end

function ‚àÇfÃÇ_correction(k,p)
    @unpack Œº, œµ, n = p
    if (fÃÇ_correction(k,p) == -Œº*œÄ*œµ^2/(2-Œº)) || Œº == 0 
        return 0
    elseif Œº!=0
        ‚àÇùîÖ‚àÇk = ForwardDiff.derivative(k -> ùîÖ(k,p),k)
        #‚àÇùîÖ‚àÇk = (-n/2)*(2œÄ*œµ/k)^(n/2)/k*besselj(n/2,k*œµ) + 
        #    (2œÄ*œµ/k)^(n/2)*œµ/2*(besselj(n/2-1,k*œµ) - besselj(n/2+1,k*œµ))
        ‚àÇ‚à´ùîâ‚àÇk = ùî£(k,p)*œµ - (n-Œº)/k * quadgk(x -> ùîâ(x,(k,p)), 0, k*œµ)[1]
        return  ‚àÇùîÖ‚àÇk - ‚àÇ‚à´ùîâ‚àÇk
    end
end


function ‚àÇ¬≤fÃÇ_correction(k,p)
    @unpack Œº, œµ, n = p
    if ‚àÇfÃÇ_correction(k,p) == 0 || Œº==0
        return 0
    elseif Œº!=0
        ‚àÇ¬≤ùîÖ‚àÇk¬≤ = ForwardDiff.derivative(k->ForwardDiff.derivative(k ->ùîÖ(k,p),k),k)
        ‚àÇ‚à´ùîâ‚àÇk = ùî£(k,p)*œµ - (n-Œº)/k * quadgk(x -> ùîâ(x,(k,p)), 0, k*œµ)[1] 
        ‚àÇ¬≤‚à´ùîâ‚àÇk¬≤ = ForwardDiff.derivative(k->ùî£(k,p),k)*œµ +
                    (n-Œº)/k^2 * quadgk(x -> ùîâ(x,(k,p)), 0, k*œµ)[1] -
                    (n-Œº)/k * ‚àÇ‚à´ùîâ‚àÇk
        return ‚àÇ¬≤ùîÖ‚àÇk¬≤ - ‚àÇ¬≤‚à´ùîâ‚àÇk¬≤
    end
end


function fzero(x, param)
    p, y = param
    return fÃÇ(x, p) - y
end

function guess_zeros(y, p)

    @unpack Œº, Œ≤, œµ, n, œÅ = p

    if Œ≤!=0
        a = 2Œº*œÄ*œµ^Œº*(1 - Œº*log(2))/(1 - Œº)*gamma(2 - Œº)
    else
        c = œÄ^(n/2)*2^Œº*gamma(Œº/2)/gamma((n-Œº)/2)
        a = (2œÄ)^n*œµ^Œº/c
    end

    b = -Œº*œÄ*œµ^2/(2-Œº)

    #guess the zeros
    if Œº!=0
        x0 = ((y .- b)/a).^(-(1/(n - Œº)))
    else
        x0 = y.^(-1/(n+1))
    end
    #=
    S = 2œÄ^(n/2)/gamma(n/2)
    df = -(2 - Œº)*a*k0.^(-(3 - Œº))
    return S/((2œÄ)^n) * k0.^(n-1)./(œÅ^2*abs.(df))
    =#
    return x0

end

function fÃÇ_inv(y,p,x0)
    #x0 is the guessed value
    # p is the parameters in the function
    # compute the inverse function of fÃÇ via root finding
    fx = ZeroProblem(fzero,x0)
    return solve(fx,(p,y))
end

function g(x,p)
    @unpack n = p
    return x^(n-1)*fÃÇ(x,p)/‚àÇfÃÇ(x,p)
end

#‚àÇg(x,p) = ForwardDiff.derivative(x -> g(x,p),x)
function ‚àÇg(x,p)
    @unpack n = p
    return (n-1)*x^(n-2)*fÃÇ(x,p)/‚àÇfÃÇ(x,p) + x^(n-1) - x^(n-1)*fÃÇ(x,p)*‚àÇ¬≤fÃÇ(x,p)/(‚àÇfÃÇ(x,p))^2
end


function Q1(x,p,Œª)
    #reformulate the integral by letting fÃÇ(k, p) -> x, and express k as fÃÇ‚Åª¬π(k, p) or x
    #thus the principal_value problem can be solved using quadgk_cauchy,
    #see the function quadgk_cauchy
    @unpack œÅ, œÉÃÑ¬≤ = p
    k0 = guess_zeros(x,p)
    k = fÃÇ_inv(x,p,k0)
    return Œª/(œÅ*œÉÃÑ¬≤)*‚àÇg(k,p)/‚àÇfÃÇ(k,p)
end


function Q2(x,p)
    #reformulate the integral by letting fÃÇ(k, p) -> x, and express k as fÃÇ‚Åª¬π(k, p) or x
    #thus the principal_value problem can be solved using quadgk_cauchy,
    #see the function quadgk_cauchy
    @unpack n, œÅ, œÉÃÑ¬≤ = p
    k0 = guess_zeros(x,p)
    k = fÃÇ_inv(x,p,k0)
    return k^(n-1)*x/‚àÇfÃÇ(k,p)/(œÅ*œÉÃÑ¬≤)
end

function quadgk_cauchy(fun, a, c, b)
    #using the transformation discussed at
    #https://github.com/JuliaMath/QuadGK.jl/pull/44#issuecomment-590606772
    fc = fun(c)
    g(x) = (fun(x) - fc) / (x - c)
    return quadgk(g, a, c, b, rtol=1e-9)[1] + fc * log(abs((b - c)/(a - c)))
end

function test_œÅ_dependence(Œª,p::ERMParameter; œÅ_min = 10, œÅ_max = 1000, N = 10)

    @unpack n, L, œÉÃÑ¬≤, œÉÃÑ‚Å¥ = p
    density = LogRange(œÅ_min,œÅ_max,N)
    P0 = similar(density)
    P1 = similar(density)
    P2 = similar(density)
    S = 2œÄ^(n/2)/gamma(n/2)
    k‚ÇÅ = œÄ/L
    k‚ÇÇ = œÄ/œµ #a choice I have not completely understood

    for j in 1:length(density)
        œÅ = density[j]
        p.œÅ = œÅ
        #k‚ÇÇ = œÄ*œÅ^(1/n)
        ùùÜ = œÅ*œÉÃÑ¬≤
        k0 = guess_zeros(Œª/ùùÜ,p)
        k = fÃÇ_inv(Œª/ùùÜ,p,k0)
        #=
        s = @sprintf "fÃÇ(k‚ÇÇ,p) is %.10f"  fÃÇ(k‚ÇÇ,p)
        println(s)

        s = @sprintf "‚àÇfÃÇ(k‚ÇÇ,p) is %.10f"  ‚àÇfÃÇ(k‚ÇÇ,p) 
        println(s)
        =#
        P‚ÇÄ = S/((2œÄ)^n) * k^(n-1)/(œÅ * ùùÜ * abs(‚àÇfÃÇ(k,p)))
        P0[j] = P‚ÇÄ
        P1[j] = 1/2 * (S/((2œÄ)^n))^2 * k^(n-1)/(ùùÜ^2 * abs(‚àÇfÃÇ(k,p))) * œÉÃÑ‚Å¥/ùùÜ *
        (-g(k‚ÇÇ,p)*Œª/(Œª - ùùÜ*fÃÇ(k‚ÇÇ,p))+
        g(k‚ÇÅ,p)*Œª/(Œª - ùùÜ*fÃÇ(k‚ÇÅ,p))+
        quadgk_cauchy(x->Q1(x,p,Œª),fÃÇ(k‚ÇÇ,p), Œª/ùùÜ, fÃÇ(k‚ÇÅ,p)))
        P2[j] = 1/2 * (S/((2œÄ)^n))^2 * œÉÃÑ‚Å¥/(ùùÜ^2*abs(‚àÇfÃÇ(k,p)))*
                ‚àÇg(k,p) * quadgk_cauchy(x->Q2(x,p),fÃÇ(k‚ÇÇ,p), Œª/ùùÜ, fÃÇ(k‚ÇÅ,p))
    end

    plot(density,P0, label="P0")
    plot!(density, -P1, label="-P1")
    plot!(density, -P2, label="-P2")
    plot!(density, P0-P1-P2, label="P0-P1-P2")
    plot!(xlabel = L"\rho", ylabel = L"\textrm{Pdf }(\lambda = %$Œª)", legend=:bottomright)
    savefig("figures/rho_dependence.png")
end

###############  Gaussian Variational Theory ######################### 

function eigendensity_variation_model(Œõ::Vector, P_sim::Vector, œÇ¬≤::Vector, p::ERMParameter; Œª_min = 1, Œª_max = 10^10)

    @unpack œÅ = p

    large_eigs = findall(x->x>=Œª_min, Œõ)
    ùú¶ = Œõ[large_eigs]
    Prob_sim = P_sim[large_eigs]
    small_eigs = findall(x->x<=Œª_max, ùú¶)
    ùú¶ = ùú¶[small_eigs]
    Prob_sim = Prob_sim[small_eigs] #simulated eigendensity, used to guess initial values
    Prob = similar(ùú¶) #predicted eigendensity from the gaussian_variation_model
    C0 = 2 #guessed value for C 

    for j in 1:length(ùú¶)
        Œª = ùú¶[j]
        C = find_zero(C->ùîä(C,œÇ¬≤,Œª,p),C0)[1]
        if !isempty(C)
            a0 = C
            b0 = Prob_sim[j]*œÅ*œÄ
            Prob[j] = gaussian_variation_model_fast(Œª,œÇ¬≤,p,a0,b0,plotroot=true)
        else
            Prob[j] = NaN
        end
        s = @sprintf "Œª = %.2f; probability density is %.5f" ùú¶[j] Prob[j]
        println(s)
    end

    return ùú¶, Prob

end

function test_œÅ_dependence_variation_model(Œª,P_sim,p::ERMParameter; œÅ_min = 10, œÅ_max = 1000, Œº = 0, œÉ = 1, N = 10)
    # Œª eigenvalue, whose density will be evaluated
    # P_sim, simulated density

    density = LogRange(œÅ_min,œÅ_max,N)
    œÇ¬≤ = vec(rand(LogNormal(Œº,œÉ),1000,1))
    P = similar(density)
    C0 = 2 #guessed value for C

    for j in 1:length(density)
        p.œÅ = density[j]
        C = find_zero(C->ùîä(C,œÇ¬≤,Œª,p),C0)

        if C > 0
            a0 = C
            b0 = P_sim*density[j]*œÄ
            P[j] = gaussian_variation_model_fast(Œª,œÇ¬≤,p,a0,b0,plotroot=true)
        else
            P[j] = NaN
        end 

        s = @sprintf "œÅ = %.3f; probability density is %.5f" density[j] P[j]
        println(s)
        
        #=
        C = find_zeros(C->ùîä(C,œÇ¬≤,Œª,p),a,b)[1]
        G_inv = G(C,p)
        s = @sprintf "G_inv is %.10f"  G_inv
        println(s)
        =#
    end
    plot(density,P)
    plot!(xlabel = L"\rho", ylabel = L"\textrm{Pdf }(\lambda = %$Œª)", legend=:bottomright)
    savefig("figures/rho_dependence_GV.png")

    return density, P

end

function gaussian_variation_model(Œª, œÇ¬≤::Vector, p::ERMParameter, a0, b0; plotroot=false)
    #input eigenvalue Œª
    #output probability density at Œª
    #œÇ¬≤ is the variance distribution of neural activity
    #a0 is the guessed real part of C, and b0 is the guessed imaginary part of C
    
    @unpack œÅ = p
    
    a_min = a0/10
    a_max = a0*10
    ax1 = Axis(range(a_min, a_max, length = 10),"a")
        
    b_max = b0*10
    b_min = b0/10
    ax2 = Axis(LogRange(b_min,b_max,10),"b")
    #a and b are real and imaginary part of C when taking the resolvant to the complex plane
    #identify the range of a and b for grid search

    mdbm1 = MDBM_Problem((a,b)->f‚Çê(a,b,Œª,œÇ¬≤,p),[ax1,ax2])
    mdbm2 = MDBM_Problem((a,b)->f·µ¶(a,b,Œª,œÇ¬≤,p),[ax1,ax2])
    MDBM.solve!(mdbm1,5)
    MDBM.solve!(mdbm2,5)
    a1, b1 = getinterpolatedsolution(mdbm1)
    a2, b2 = getinterpolatedsolution(mdbm2)
    if plotroot
        scatter(a1,b1)
        scatter!(a2,b2)
        savefig("figures/test_root_finding")
    end
    #find solutions: f‚Çê(a,b,Œª,œÇ¬≤,p) = 0
    #find solutions: f·µ¶(a,b,Œª,œÇ¬≤,p) = 0
    if isempty(a1) || isempty(a2)
        return NaN #did not find solutions in the specified range
    else
        intersection_pts = find_intersection_between_curves((a1,b1),(a2,b2))

        if !isempty(intersection_pts)
            a_sol, b_sol = intersection_pts[1]
            return find_density(a_sol,b_sol,Œª,œÇ¬≤,p)
        else
            return NaN #did not find a solution
        end
    end

end

function find_intersection_between_curves(curve1, curve2)
    
    (X1,Y1)=curve1
    (X2,Y2)=curve2

    N1 = length(X1)
    N2 = length(X2)

    D¬≤ = (repeat(X1,1,N2)' .- X2).^2 + (repeat(Y1, 1, N2)' .- Y2).^2
    #compute pairwise distance between all points from the two curves
    
    indices = findall(d¬≤ -> d¬≤ < 1e-1, D¬≤)
    #find all potential intersection regions

    intersections = []

    for n in 1:length(indices)
        i,j = Tuple(indices[n])
        if i < N2 && j < N1
            line1 = ((X1[j],Y1[j]),(X1[j+1],Y1[j+1])) #(start point, end point)
            line2 = ((X2[i],Y2[i]),(X2[i+1],Y2[i+1])) #(start point, end point)
            x,y = find_intersection_between_lines(line1,line2)
            if !isempty(x)
                if (X1[j] <= x <= X1[j+1]) || (X1[j] >= x >= X1[j+1])
                    push!(intersections,(x,y))
                end
            end
        end
    end
    
    return intersections

end

function find_intersection_between_lines(line1,line2)
    pt_start1, pt_end1 = line1
    pt_start2, pt_end2 = line2
    k1 = (pt_end1[2] - pt_start1[2])/(pt_end1[1] - pt_start1[1])
    k2 = (pt_end2[2] - pt_start2[2])/(pt_end2[1] - pt_start2[1])
    if k1==k2
        return []
    else
        x_sol = (k1*pt_start1[1] - k2*pt_start2[1] + pt_start2[2] - pt_start1[2])/(k1 - k2)
        y_sol = k1*(x_sol - pt_start1[1]) + pt_start1[2]
        return x_sol, y_sol
    end
end

function g·µ£(a,b,p::ERMParameter)
    @unpack n, L, œÅ, œµ = p
    S = 2œÄ^(n/2)/gamma(n/2)
    #k‚ÇÇ = œÄ*œÅ^(1/n)
    k‚ÇÇ = œÄ/œµ #*100
    k‚ÇÅ = œÄ/L
    #k‚ÇÅ = 10^-12
    return S/(2œÄ)^n*quadgk(k->k^(n-1)*fÃÇ(k,p)*(1-a*fÃÇ(k,p))/((1-a*fÃÇ(k,p))^2 + b^2*fÃÇ(k,p)^2),k‚ÇÅ,k‚ÇÇ)[1]
end

function g·µ¢(a,b,p::ERMParameter)
    @unpack n, L, œÅ, œµ = p
    S = 2œÄ^(n/2)/gamma(n/2)
    #k‚ÇÇ = œÄ*œÅ^(1/n)
    k‚ÇÇ = œÄ/œµ #*100
    k‚ÇÅ = œÄ/L
    #k‚ÇÅ = 10^-12
    return S/(2œÄ)^n*quadgk(k->k^(n-1)*b*fÃÇ(k,p)^2/((1-a*fÃÇ(k,p))^2 + b^2*fÃÇ(k,p)^2),k‚ÇÅ,k‚ÇÇ)[1]
end

function f‚Çê(a,b,Œª,œÇ¬≤::Vector,p::ERMParameter)
    @unpack œÅ = p
    #ReC = œÅ*œÇ¬≤ .* (Œª .- œÇ¬≤*g·µ£(a,b,p)) ./ ((Œª .- œÇ¬≤*g·µ£(a,b,p)).^2 .+ g·µ¢(a,b,p)^2)
    ReC = œÅ*œÇ¬≤ .* (Œª .- œÇ¬≤*g·µ£(a,b,p)) ./ ((Œª .- œÇ¬≤*g·µ£(a,b,p)).^2 .+ (œÇ¬≤*g·µ¢(a,b,p)).^2)
    return mean(ReC) - a 
end

function f·µ¶(a,b,Œª,œÇ¬≤::Vector,p::ERMParameter)
    @unpack œÅ = p
    #ImC = œÅ*œÇ¬≤*g·µ¢(a,b,p) ./ ((Œª .- œÇ¬≤*g·µ£(a,b,p)).^2 .+ g·µ¢(a,b,p)^2)
    ImC = œÅ*œÇ¬≤.*œÇ¬≤*g·µ¢(a,b,p) ./ ((Œª .- œÇ¬≤*g·µ£(a,b,p)).^2 .+ (œÇ¬≤ *g·µ¢(a,b,p)).^2)
    return mean(ImC) - b
end

function find_density(a,b,Œª,œÇ¬≤::Vector,p::ERMParameter)
    #return mean(g·µ¢(a,b,p) ./ ((Œª .- œÇ¬≤*g·µ£(a,b,p)).^2 .+ g·µ¢(a,b,p)^2))/œÄ
    return mean(œÇ¬≤*g·µ¢(a,b,p) ./ ((Œª .- œÇ¬≤*g·µ£(a,b,p)).^2 .+ (œÇ¬≤*g·µ¢(a,b,p)).^2))/œÄ
end

function gaussian_variation_model_fast(Œª, œÇ¬≤::Vector, p::ERMParameter, a0, b0; plotroot=false)
    #input eigenvalue Œª
    #output probability density at Œª
    #œÇ¬≤ is the variance distribution of neural activity
    #a0 is the guessed real part of C, and b0 is the guessed imaginary part of C
    
    @unpack œÅ = p

    b = 1
    a = 1
    for i = 1:10
        ffa(a0) = f‚Çê(a0,b,Œª,œÇ¬≤,p)
        a = find_zero(ffa,a)
        ffb(b0) = f·µ¶(a,b0,Œª,œÇ¬≤,p)
        b = find_zero(ffb,b,Order1())
    end

    return find_density(a,b,Œª,œÇ¬≤,p)
    
end

function gÃÇ(x,C,p)
    @unpack n = p
    k0 = guess_zeros(x,p)
    k = fÃÇ_inv(x,p,k0)
    S = 2œÄ^(n/2)/gamma(n/2)
    return S/(2œÄ)^n*k^(n-1)/‚àÇfÃÇ(k,p)*x/C
end

function GÃÇ(k,C,p) 
    @unpack n = p
    S = 2œÄ^(n/2)/gamma(n/2)
    return S/(2œÄ)^n*k^(n-1)*fÃÇ(k,p)/(1-C*fÃÇ(k,p))
end


function G(C,p)
    @unpack n, L, œÅ, œµ = p
    #k‚ÇÇ = œÄ*œÅ^(1/n)
    k‚ÇÇ = œÄ/œµ
    k‚ÇÅ = œÄ/L
    if fÃÇ(k‚ÇÇ,p) < 1/C <  fÃÇ(k‚ÇÅ,p)
        G_inv = quadgk_cauchy(x->gÃÇ(x,C,p),fÃÇ(k‚ÇÇ,p), 1/C, fÃÇ(k‚ÇÅ,p))
        #G_inv = ‚à´dxgÃÇ(x)/(x-1/C)
    else
        G_inv = quadgk(k->GÃÇ(k,C,p),k‚ÇÅ,k‚ÇÇ)[1]
        #G_inv = ‚à´dkfÃÇ(k)/(1-CfÃÇ(k))
    end
    return G_inv
end

function ùîä(C,œÇ¬≤::Vector,Œª,p)
    #sampling method
    @unpack œÅ = p
    return mean(œÅ*œÇ¬≤ ./ (Œª .- œÇ¬≤*G(C,p))) - C
end

#=
function ùîä(C,Œº,œÉ,Œª,p)
    #direct integral
    @unpack œÅ = p
    œÇ¬≤_min = 0
    œÇ¬≤_max = 30
    return quadgk(œÇ¬≤->lognormal(œÇ¬≤,Œº = Œº, œÉ = œÉ)*œÅ*œÇ¬≤/(Œª - œÇ¬≤*G(C,p)),œÇ¬≤_min,œÇ¬≤_max)[1] - C
end
=#


lognormal(x; Œº = 0, œÉ = 1) = 1/(x*‚àö(2œÄ*œÉ^2))*exp(-(log(x)-Œº)^2/(2œÉ^2))

#=
function principal_value(fun, a, b, pole; œµ = 1e-6)
    if a<pole<b
        integral1, err = quadgk(fun, a, pole-œµ, rtol=1e-8)
        integral2, err = quadgk(fun, pole+œµ, b, rtol=1e-8)
        return integral1+integral2
    else
        integral, err = quadgk(fun, a, b, rtol=1e-8)
        return integral
    end
end
=#


LogRange(x1, x2, n) = 10 .^ range(log10(x1), log10(x2), length=n)


function construct_grid_points(N; dims = 3)
    points = zeros(N^dims, dims)
    xs = LinRange(-N/2,N/2,N)
    ys = LinRange(-N/2,N/2,N)
    zs = LinRange(-N/2,N/2,N)
    p = 0
    for j in xs
        for i in ys
            for k in zs
                p+=1
                points[p,:]=[i j k]
            end
        end
    end
    return points
end

function model(dim, d, Œ±; f = "normal")
    global p = 0
    plot(p)
    for i = 6:10
    n = 2^i
    if f == "normal"
        x = randn(n,dim)
    elseif f == "uniform"
        x = rand(n,dim)
    end
    l = zeros(n,n)
    for i = 1:n
        for j = i:n
            l[i,j] = l[j,i] = sqrt(sum(abs2, x[j,:] - x[i,:]))
        end
    end
    c = (l.+d).^(-Œ±)

    eig = sort(eigvals(c), rev=true)
    plot!((1:n)/n, eig, xaxis=:log, yaxis=:log)
    global p =plot!((1:n)/n, eig, xaxis=:log, yaxis=:log) 
    end

    plot(p)
    png("figures/"*"power"*"_normal"*"_$d"*"_$Œ±")
end

## convert Covariance matrix to Distance matrix with customized function

function cov2dmat(G::AbstractMatrix{T}; œµ = 0.1f0, Œº = 0.3f0) where {T<:Real}
    # argument checking
    m = size(G, 1)
    n = size(G, 2)
    m == n || error("D should be a square matrix.")
    
    D = similar(G,T)

    # implementation
    for j = 1:n
        for i = 1:j-1
            @inbounds D[i,j] = D[j,i]
        end
        D[j,j] = zero(T)
        for i = j+1:n
            Œ∑ = abs((G[i,i] + G[j,j])/(2*G[i,j]))
            @inbounds D[i,j] = œµ*(Œ∑^(1/Œº)-1)
        end
    end
    return D
end

function replace_matrix_diag(C)
    N = size(C,1)
    Œî = diag(C)
    Œ¥ = shuffle(Œî)
    return C - diagm(Œî) + diagm(Œ¥)
end

function find_nearest_neighbor(D)
    n = size(D,1) 
    d = zeros(n-1)
    for j = 1:n-1
        d[j]=minimum(D[j,j+1:n])
    end
    return d
end

function hirarchical_clustering(C;k=2)
    D = gram2dmat(C)
    result = hclust(D, linkage=:single)
    z = cutree(result,k=k) 
    return result, z 
end

function activity_distribution(C; n = 20)
    var = diag(C)
    xrange = LogRange(minimum(var), maximum(var), n)
    h = fit(Histogram,var,xrange)
    h = normalize(h, mode=:pdf)
    x, p = dropzero(h)
    plot(x,p, markershape=:circle, xaxis = :log)
    png("figures/variance_distribution.png")
end


function gaussian_variation_model_fast_ab(Œª, œÇ¬≤::Vector, p::ERMParameter, a0, b0; plotroot=false)
    #input eigenvalue Œª
    #output probability density at Œª
    #œÇ¬≤ is the variance distribution of neural activity
    #a0 is the guessed real part of C, and b0 is the guessed imaginary part of C
    
    @unpack œÅ = p

    b = 1
    a = 1
    for i = 1:10
        ffa(a0) = f‚Çê(a0,b,Œª,œÇ¬≤,p)
        a = find_zero(ffa,a)
        ffb(b0) = f·µ¶(a,b0,Œª,œÇ¬≤,p)
        b = find_zero(ffb,b,Order1())
    end

    return find_density(a,b,Œª,œÇ¬≤,p), a, b
    
end

function mdscale(D; n=2, criterion = "sammon", weight = nothing, Start = "cmdscale")
    if weight == nothing
        mat"""
        [$X, $stress]= mdscale([$D],[$n], 'criterion',[$criterion],'Start',[$Start]); 
        """
    else
        mat"""
        [$X, $stress]= mdscale([$D],[$n], 'criterion',[$criterion],'weight',[$weight],'Start',[$Start]); 
        """
    end
    return X', stress
end

function umeyama(X,Y)
    N = size(X,2)
    d = size(X,1)
    if d == 2
        X2 = vcat(X,zeros(1,N))
        Y2 = vcat(Y,zeros(1,N))
    end
    mat"""
    [$R, $t]= umeyama(X2, Y2); 
    """
    if d == 2
        R = R[1:2,1:2]
        t = t[1:2]
    end
    return R, t
end

function Parameter_estimation(C; œµ = 0.03125, bin = 0:0.01:1, bin_fit = 0.1:0.01:0.5, n = 2)
    cd1 = max.(0,reshape(C,:)) 
    S = 2œÄ^(n/2)/gamma(n/2)
    h = StatsBase.fit(Histogram, cd1, bin)
    h = normalize(h, mode = :pdf)
    cor, p_corr = dropzero(h)
    a, b = log_fit(cor[10:end-10], log.(p_corr[10:end-10]))
    Œº = -n/(1+b)
    N = size(C)[1]
    œÅ = N*Œº*exp(a)/(S*œµ^n)
    return œÅ, Œº
end

function power_exp(r; œµ = 0.01, Œæ = 1, Œº = 0.3)
    c = (r/œµ)^(-Œº) * exp(-(r-œµ)/Œæ)
    return c
end

function inv_f(c, œµ, Œæ, Œº)
    p_e(r) = power_exp(r; œµ = œµ, Œæ = Œæ, Œº = Œº)-c
    r = find_zero(p_e,0.0001, Order1())
    return r
end

function Parameter_estimation2(C; œµ = 0.03125, Œº = nothing, bin = 0:0.01:1, st = 10, distribution = "uniform")
    cd1 = max.(0,reshape(C,:)) 
    #cd1 = abs.(reshape(C,:))
    h = StatsBase.fit(Histogram, cd1, bin)
    h = normalize(h, mode = :pdf)
    function loss(x, a0; Œº = Œº, n = 3086, œµ = 0.01)
        p, c = x
        if Œº == nothing
            œÅ, Œæ, Œº = a0
        else
            œÅ, Œæ = a0
        end
        rr = inv_f(c, œµ, Œæ, Œº)
        if distribution == "uniform"
            l1= -log(p)
            l2 = - log(c/2/pi)
            l3 = + 2*log(rr)
            l4 = - log((Œº +rr/Œæ))
            œÅ = max(œÅ, 10)
            l5 = - log(n/œÅ)
            l = l1+l2+l3+l4+l5
            #l = -log(p) - log(c/2/pi) + 2*log(rr) - log((Œº +rr/Œæ)*n/œÅ)
        elseif distribution == "normal"
            l = -log(p) - log(c*2) + 2*log(rr) - log((Œº +rr/Œæ)*n/œÅ) - rr^2/(4*n/œÅ)
        end
        return l
    end
    x = hcat(h.weights[st:end], bin[st:end-1])
    x = hcat(h.weights[st:50], bin[st:50])
    if Œº == nothing
        a0 = [1000, 0.1, 0.3]
        coefs, converged, iter = nonlinear_fit(x, loss, a0, 1e-3, 200)
        œÅ, Œæ, Œº = coefs
    else
        a0 = [1000, 0.1]
        coefs, converged, iter = nonlinear_fit(x, loss, a0, 1e-3, 200)
        œÅ, Œæ = coefs
    end
    return œÅ, Œæ, Œº
end


function diff_gaussian_variation_model(Œª; p = nothing, œÇ¬≤ = nothing, dœÅ = 10^-6, a0 = 1, b0 = 1)
    if p == nothing
        N = 1024; L = 10; œÅ = 5.12; n = 2; œµ = 0.03125; Œº = 0.5; Œæ = 10
    else
        @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    end
    if œÇ¬≤ == nothing
        œÇ¬≤ = vec(ones(N,1))
    end
    p0 = ERMParameter(;N = N, L = L, œÅ = œÅ, n = n, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ =Œ≤, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    P_Œª = gaussian_variation_model_fast(Œª,œÇ¬≤,p0,a0,b0,plotroot=true)
    p1 = ERMParameter(;N = N, L = L, œÅ = œÅ+dœÅ, n = n, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ =Œ≤, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    P_Œª1 = gaussian_variation_model_fast(Œª,œÇ¬≤,p1,a0,b0,plotroot=true)
    dP_ŒªdœÅ = (P_Œª1 - P_Œª)/dœÅ
    return dP_ŒªdœÅ/P_Œª*œÅ  # dlnp/dlnœÅ
    #return dP_Œª 
end

function diff_gaussian_variation_theory(Œª;N = 1024, L = 10, œÅ = 5.12, n = 2, œµ = 0.03125, Œº = 0.5, 
    Œæ = 10, œÇ¬≤ = nothing, dœÅ = 10^-6, a0 = 1, b0 = 1)
    if œÇ¬≤ == nothing
        œÇ¬≤ = vec(ones(N,1))
    end
    p0 = ERMParameter(;N = N, L = L, œÅ = œÅ, n = d, œµ = œµ, Œº = Œº, Œæ = Œæ, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    P_Œª, a, b = gaussian_variation_model_fast_ab(Œª,œÇ¬≤,p0,a0,b0,plotroot=true)
    p1 = ERMParameter(;N = N, L = L, œÅ = œÅ+dœÅ, n = d, œµ = œµ, Œº = Œº, Œæ = Œæ, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    P_Œª1, a1, b1 = gaussian_variation_model_fast_ab(Œª,œÇ¬≤,p1,a0,b0,plotroot=true)
    dg·µ¢dœÅ = (g·µ¢(a1,b1,p1) - g·µ¢(a,b,p0))/dœÅ
    dg·µ£dœÅ = (g·µ£(a1,b1,p1) - g·µ£(a,b,p0))/dœÅ
    dpdœÅ = dg·µ¢dœÅ .* œÇ¬≤ ./ (Œª .- œÇ¬≤.* g·µ£(a,b,p0)).^2 .+ dg·µ£dœÅ .* g·µ¢(a,b,p0) .* œÇ¬≤.^2 ./ (Œª .- œÇ¬≤.* g·µ£(a,b,p0)).^3
    return  mean(dpdœÅ/P_Œª)/œÄ*œÅ
end

function diff_high_density_model(Œª;N = 1024, L = 10, œÅ = 5.12, n = 2, œµ = 0.03125, Œº = 0.5, 
    Œæ = 10, œÇ¬≤ = nothing, dœÅ = 10^-6, a0 = 1, b0 = 1)
    if œÇ¬≤ == nothing
        œÇ¬≤ = vec(ones(N,1))
    end
    œÉÃÑ¬≤ = mean(œÇ¬≤)
    p0 = ERMParameter(;N = N, L = L, œÅ = œÅ, n = d, œµ = œµ, Œº = Œº, Œæ = Œæ, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    _, P_Œª = eigendensity(Œª/œÉÃÑ¬≤,p0, Œª_min = 0)
    P_Œª = P_Œª/œÉÃÑ¬≤
    p1 = ERMParameter(;N = N, L = L, œÅ = œÅ+dœÅ, n = d, œµ = œµ, Œº = Œº, Œæ = Œæ, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    _, P_Œª1 = eigendensity(Œª/œÉÃÑ¬≤,p1, Œª_min = 0)
    P_Œª1 = P_Œª1/œÉÃÑ¬≤
    dP_Œª = (P_Œª1 - P_Œª)./dœÅ
    return dP_Œª./P_Œª*œÅ
    #return dP_Œª 
end

function eigendensity_kernel(C::Matrix; length = 100, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10^10,
    m = 1, different_neural_variability = false, theory = true, œÉ¬≤ = nothing, bandwidth = nothing, N_sample = nothing)
    
    N_all = size(C)[1]
    if N_sample ==nothing
        N_sample = N_all
    end

    Œª_all = zeros(m,N_sample)
    for i = 1:m
        id = sample(1:N_all,N_sample, replace = false)
        Œª = eigvals(C[id,id])
        Œª = max.(Œª,10^-36)
        Œª_all[i,:] = Œª
    end

    Œª = reshape(Œª_all,N_sample*m)

    if bandwidth == nothing
        y = kde(Œª)
    else
        y = kde(Œª, bandwidth = bandwidth)
    end

    return y, Œª
end

function eigendensity_kernel_log(C::Matrix; length = 100, correction = false, use_variation_model = false, Œª_min = 1, Œª_max = 10^10,
    m = 1, different_neural_variability = false, theory = true, œÉ¬≤ = nothing, bandwidth = nothing, N_sample = nothing)

    N_all = size(C)[1]
    if N_sample ==nothing
        N_sample = N_all
    end

    Œª_all = zeros(m,N_sample)
    for i = 1:m
        id = sample(1:N_all,N_sample, replace = false)
        Œª = eigvals(C[id,id])
        Œª = max.(Œª,10^-36)
        Œª_all[i,:] = Œª
    end

    Œª = reshape(Œª_all,N_sample*m)

    if bandwidth == nothing
        y = kde(log.(Œª))
    else
        y = kde(log.(Œª), bandwidth = bandwidth)
    end

    return y, Œª
end


function pdf_log_kernal(y,x)
    return pdf(y,log.(x)) ./ x
end


function dlnpdlnœÅ_fit(C, Œª; fit_length = 10, N_sample = [1000,500], bandwidth = nothing, m = 1)
    fit_length = fit_length
    N_sample_fit = LogRange(N_sample[1],N_sample[2],fit_length)
    N_sample_fit = Int.(round.(N_sample_fit))
    p_all = zeros(fit_length,length(Œª))
    for i = 1:fit_length
        y, _ = eigendensity_kernel_log(C, m = m, N_sample = N_sample_fit[i], bandwidth = bandwidth)
        p_all[i,:] .= pdf_log_kernal(y,Œª)
    end
    
    dlnpdlnœÅ = zeros(size(Œª))
    for i = 1:length(Œª)
        ft = curve_fit(LinearFit, log.(N_sample_fit), log.(p_all[:,i]))
        dlnpdlnœÅ[i] = ft(1) - ft(0)
    end
    return dlnpdlnœÅ
end

function dlnpdlnœÅ_fit(p::ERMParameter, Œª; œÉ = nothing, fit_length = 10, N_sample = [1000,500], bandwidth = nothing, m = 1)
    fit_length = fit_length
    N_sample_fit = LogRange(N_sample[1],N_sample[2],fit_length)
    N_sample_fit = Int.(round.(N_sample_fit))
    p_all = zeros(fit_length,length(Œª))
    for i = 1:fit_length
        points_i = rand(Uniform(-L/2,L/2),N_sample[1],d)
        C_i = reconstruct_covariance(points_i, p, subsample = false)
        if œÉ != nothing
            #œÉ_i = sample(œÉ,N_sample_fit[i], replace = false)
            Œî_i = diagm(œÉ)
            C_i = Œî_i * C_i * Œî_i
        end
        C_i = (C_i+C_i')/2
        y, _ = eigendensity_kernel_log(C_i, m = m, N_sample = N_sample_fit[i], bandwidth = bandwidth)
        p_all[i,:] .= pdf_log_kernal(y,Œª)
    end
    
    dlnpdlnœÅ = zeros(size(Œª))
    for i = 1:length(Œª)
        ft = curve_fit(LinearFit, log.(N_sample_fit), log.(p_all[:,i]))
        dlnpdlnœÅ[i] = ft(1) - ft(0)
    end
    return dlnpdlnœÅ
end

#=
function collapse_index_num(C; Œª_max = 10, Œª_min = 1, dŒª = 10^-3, m = 32, bandwidth = nothing, N_sample = nothing, use_log = true, fit_length = 2)
    #Œª_corr, p_corr = eigendensity_range(Corr, Œª_min = 0.1, Œª_max = 100, length = 20)
    #y, Œª = eigendensity_kernel(Corr)
    #N_c = size(C)[1]
    #N_c2 = Int(round(N_c/2))
    if N_sample == nothing
        N_sample1 = size(C)[1]
        N_sample2 = Int(round(size(C)[1] /2))
        N_sample = [N_sample1, N_sample2]
    end
    N = size(C)[1]
    if N < N_sample[1]
        N_sample[1] = N
    end
    Œª = Œª_min:dŒª:Œª_max
    if use_log 
        #y1, _ = eigendensity_kernel_log(C, m = m, N_sample = N_sample[1], bandwidth = bandwidth)
        #y2, _ = eigendensity_kernel_log(C, m = m, N_sample = N_sample[2], bandwidth = bandwidth)
        #p1 = pdf_log_kernal(y1,Œª)
        #p2 = pdf_log_kernal(y2,Œª)
        dlnpdlnœÅ = dlnpdlnœÅ_fit(C, Œª; fit_length = fit_length, N_sample = N_sample, bandwidth = bandwidth, m = m)
    elseif !use_log
        points_i = rand(Uniform(-L/2,L/2),N_sample_fit[i],d)
        C_i = reconstruct_covariance(points1, p, subsample = false)
        y1, _ = eigendensity_kernel(C_i, m = m, N_sample = N_sample[1], bandwidth = bandwidth)
        y2, _ = eigendensity_kernel(C_i, m = m, N_sample = N_sample[2], bandwidth = bandwidth)
        p1 = max.(pdf(y1,Œª),0)
        p2 = max.(pdf(y2,Œª),0)
        dlnpdlnœÅ = (log.(p1)-log.(p2) )/ log(N_sample[1]/N_sample[2])
    end
    index = mean(abs.(dlnpdlnœÅ)) *(Œª_max-Œª_min)
    return index, dlnpdlnœÅ, Œª 
end
=#

function collapse_index_num(C; Œª_max = 10, Œª_min = 1, dŒª = 10^-3, m = 32, bandwidth = nothing, N_sample = nothing, use_log = true, fit_length = 2)
    #Œª_corr, p_corr = eigendensity_range(Corr, Œª_min = 0.1, Œª_max = 100, length = 20)
    #y, Œª = eigendensity_kernel(Corr)
    #N_c = size(C)[1]
    #N_c2 = Int(round(N_c/2))
    if N_sample == nothing
        N_sample1 = size(C)[1]
        N_sample2 = Int(round(size(C)[1] /2))
        N_sample = [N_sample1, N_sample2]
    end
    N = size(C)[1]
    if N < N_sample[1]
        N_sample[1] = N
    end
    Œª = Œª_min:dŒª:Œª_max
    if use_log 
        #y1, _ = eigendensity_kernel_log(C, m = m, N_sample = N_sample[1], bandwidth = bandwidth)
        #y2, _ = eigendensity_kernel_log(C, m = m, N_sample = N_sample[2], bandwidth = bandwidth)
        #p1 = pdf_log_kernal(y1,Œª)
        #p2 = pdf_log_kernal(y2,Œª)
        dlnpdlnœÅ = dlnpdlnœÅ_fit(C, Œª; fit_length = fit_length, N_sample = N_sample, bandwidth = bandwidth, m = m)
    elseif !use_log
        points_i = rand(Uniform(-L/2,L/2),N_sample_fit[i],d)
        C_i = reconstruct_covariance(points1, p, subsample = false)
        y1, _ = eigendensity_kernel(C_i, m = m, N_sample = N_sample[1], bandwidth = bandwidth)
        y2, _ = eigendensity_kernel(C_i, m = m, N_sample = N_sample[2], bandwidth = bandwidth)
        p1 = max.(pdf(y1,Œª),0)
        p2 = max.(pdf(y2,Œª),0)
        dlnpdlnœÅ = (log.(p1)-log.(p2) )/ log(N_sample[1]/N_sample[2])
    end
    index = mean((abs.(dlnpdlnœÅ))./Œª) *(Œª_max-Œª_min)
    return index, dlnpdlnœÅ, Œª 
end

function collapse_index_sim(p; œÉ = nothing, Œª_max = 10, Œª_min = 1, dŒª = 10^-3, m = 32, bandwidth = nothing, N_sample = nothing, use_log = true, fit_length = 2)
    if N_sample == nothing
        N_sample1 = p.N
        N_sample2 = Int(round(p.N /2))
        N_sample = [N_sample1, N_sample2]
    end
    Œª = Œª_min:dŒª:Œª_max
    if use_log 
        dlnpdlnœÅ = dlnpdlnœÅ_fit(p, Œª; œÉ = œÉ, fit_length = fit_length, N_sample = N_sample, bandwidth = bandwidth, m = m)
    elseif !use_log
        y1, _ = eigendensity_kernel(C1, m = m, N_sample = N_sample[1], bandwidth = bandwidth)
        y2, _ = eigendensity_kernel(C2, m = m, N_sample = N_sample[2], bandwidth = bandwidth)
        p1 = max.(pdf(y1,Œª),0)
        p2 = max.(pdf(y2,Œª),0)
        dlnpdlnœÅ = (log.(p1)-log.(p2) )/ log(N_sample[1]/N_sample[2])
    end
    index = mean(abs.(dlnpdlnœÅ)) *(Œª_max-Œª_min)
    return index, dlnpdlnœÅ, Œª 
end

#=
function collapse_index_theory(p; Œª_max = 10, Œª_min = 1, dŒª = 10^-3, m = 32, bandwidth = nothing, N_sample = nothing, len = 10, œÇ¬≤ = nothing)
    #Œª_corr, p_corr = eigendensity_range(Corr, Œª_min = 0.1, Œª_max = 100, length = 20)
    #y, Œª = eigendensity_kernel(Corr)
    #N_c = size(C)[1]
    #N_c2 = Int(round(N_c/2))
    Œª_all =  Œª_min:(Œª_max - Œª_min) / (len - 1):Œª_max
    dlnP_Œª_all_corr = diff_gaussian_variation_model.(Œª_all, p = p, œÇ¬≤ = œÇ¬≤)

    A = hcat(Œª_all,dlnP_Œª_all_corr)
    itp = Interpolations.scale(interpolate(A, (BSpline(Cubic(Natural(OnGrid()))), NoInterp())), Œª_all, 1:2)

    tfine = Œª_min:dŒª:Œª_max
    Œª, dlnpdlnœÅ = [itp(t,1) for t in tfine], [itp(t,2) for t in tfine]

    index = mean(abs.(dlnpdlnœÅ)) *(Œª_max-Œª_min)
    return index, dlnpdlnœÅ, Œª 
end
=#

function collapse_index_theory(p; Œª_max = 10, Œª_min = 1, dŒª = 10^-3, m = 32, bandwidth = nothing, N_sample = nothing, len = 10, œÇ¬≤ = nothing)
    #Œª_corr, p_corr = eigendensity_range(Corr, Œª_min = 0.1, Œª_max = 100, length = 20)
    #y, Œª = eigendensity_kernel(Corr)
    #N_c = size(C)[1]
    #N_c2 = Int(round(N_c/2))
    Œª_all =  Œª_min:(Œª_max - Œª_min) / (len - 1):Œª_max
    dlnP_Œª_all_corr = diff_gaussian_variation_model.(Œª_all, p = p, œÇ¬≤ = œÇ¬≤)

    A = hcat(Œª_all,dlnP_Œª_all_corr)
    itp = Interpolations.scale(interpolate(A, (BSpline(Cubic(Natural(OnGrid()))), NoInterp())), Œª_all, 1:2)

    tfine = Œª_min:dŒª:Œª_max
    Œª, dlnpdlnœÅ = [itp(t,1) for t in tfine], [itp(t,2) for t in tfine]

    index = mean((abs.(dlnpdlnœÅ))./Œª) *(Œª_max-Œª_min)
    return index, dlnpdlnœÅ, Œª 
end

function collapse_index_theory2(p; p0 = 0.01, Œª_max = 10, Œª_min = 1, dŒª = 10^-3, m = 32, bandwidth = nothing, N_sample = nothing, len = 10, œÇ¬≤ = nothing)
    #Œª_corr, p_corr = eigendensity_range(Corr, Œª_min = 0.1, Œª_max = 100, length = 20)
    #y, Œª = eigendensity_kernel(Corr)
    #N_c = size(C)[1]
    #N_c2 = Int(round(N_c/2))
    @unpack L, n, N = p 
    points = rand(Uniform(-L/2,L/2),Int(N/2),d)
    C = reconstruct_covariance(points, p, subsample = false)
    lb_N = -sort(-eigvals(Hermitian(C)))
    Œª_max = lb_N[Int(round(p0*N/2))]
    Œª_all =  Œª_min:(Œª_max - Œª_min) / (len - 1):Œª_max
    dlnP_Œª_all_corr = diff_gaussian_variation_model.(Œª_all, p = p, œÇ¬≤ = œÇ¬≤)

    A = hcat(Œª_all,dlnP_Œª_all_corr)
    itp = Interpolations.scale(interpolate(A, (BSpline(Cubic(Natural(OnGrid()))), NoInterp())), Œª_all, 1:2)

    tfine = Œª_min:dŒª:Œª_max
    Œª, dlnpdlnœÅ = [itp(t,1) for t in tfine], [itp(t,2) for t in tfine]

    index = mean((abs.(dlnpdlnœÅ))./Œª) *(Œª_max-Œª_min) / log(Œª_max/Œª_min)
    return index, dlnpdlnœÅ, Œª 
end

function collapse_index_rank(C; p0 = 0.01)
    
    s = 0.5 # subsampling fraction
    #p0 = 0.01 # lambda_max cutoff

    N,_ = size(C)
    Ns = round(Int, N*s)
    lb_N = -sort(-eigvals(Hermitian(C)))
    ntrial = 2000
    lb_Ns = zeros(Ns,ntrial)
    Threads.@threads for i in 1:ntrial
        i_s = randperm(N)[1:Ns]
        Cs = copy(C[:,i_s])
        Cs = Cs[i_s,:]
        lb_Ns[:,i] = -sort(-eigvals(Hermitian(Cs)))
    end
    lb_Ns = vec(mean(lb_Ns,dims=2))

    r0 = round(Int, Ns*p0-0.5) # lambda_max cutoff
    r1 = maximum(findall(lb_Ns .> 1)) # lambda_min cutoff
    f_Ns = log.(lb_Ns[r0+1:r1])
    f_N = zeros(length(f_Ns))
    for k in 1:length(f_N)
        i = r0 + k
        j = (i)/Ns*N
        if isinteger(j)
            j = convert(Int,j)
            f_N[k] = log.(lb_N[j])
        else
            # interpolation in loglog scale
            j0 = convert(Int,floor(j))
            # f_N[k] = (j0+1-j)*lb_N[j0] + (j-j0)*lb_N[j0+1]
            #=
            x = np.log(j/N)
            x0 = np.log(j0/N)
            x1 = np.log((j0+1)/N)
            y0 = np.log(lb_N[j0])
            y1 = np.log(lb_N[j0+1])
            =#
            x = log(j/N)
            x0 = log(j0/N)
            x1 = log((j0+1)/N)
            y0 = log(lb_N[j0])
            y1 = log(lb_N[j0+1])
            f_N[k] = (y0*(x1-x) + y1*(x-x0)) / (x1-x0)
        end
    end
    S = 0
    for k in 1:length(f_N)-1
        i = r0 + k
        a = (i)/Ns
        b = (i+1)/Ns
        y1 = abs(f_N[k] - f_Ns[k])
        y2 = abs(f_N[k+1] - f_Ns[k+1])
        S += log(b/a)*(y1+y2)/2
    end
    CI = S/log(N/Ns)/log((r1)/(r0+1))
    return CI, (r0+1)/Ns, r1/Ns
end

function loss_mds(C,p)
    D2 = pairwise(Euclidean(), X', dims=1)
    C2 = corr(D2,p)  
    loss = Flux.mse(C,C2)
    return loss
end


function loss_mds(C)
    D2 = pairwise(Euclidean(), X', dims=1)
    loss = Flux.mse(C,D2)
    return loss
end


function pairwise_dot_kai(x)
    d, n = size(x)
    xixj = x' * x
    xsq = sum(x .^ 2; dims=1)
    return repeat(xsq, n, 1) + repeat(xsq', 1, n) - 2xixj
end

dis(x) = @tullio C[i,j] := x[k, i] ^ 2 - 2 * x[k, i] * x[k, j] + x[k, j] ^ 2

function loss_mds(C,p)
    D2 = sqrt.(abs.(dis(X)))
    C2 = corr(D2,p)  
    loss = Flux.mse(C,C2)
    return loss
end

function G_int(z, p; C = nothing)
    @unpack L, œµ, œÅ, n = p
    k‚ÇÅ = œÄ/L
    k‚ÇÇ = œÄ/œµ 
    a = fÃÇ(k‚ÇÅ, p)
    b = fÃÇ(k‚ÇÇ, p)
    if C == nothing
        c = z/œÅ
    else
        c = 1/C
    end
    #fun(x) = -2*œÄ*fÃÇ_inv(x,p,0.1)*x*z/œÅ/‚àÇfÃÇ(fÃÇ_inv(x,p,0.1),p) / (2*œÄ)^2
    fun(x) = -2*œÄ*fÃÇ_inv(x,p,guess_zeros(x,p))*x*c/‚àÇfÃÇ(fÃÇ_inv(x,p,guess_zeros(x,p)),p) / (2*œÄ)^2
    if (c>a&&c<b)||(c<a&&c>b)
        Œª = quadgk_cauchy(fun, a, c, b)
    else
        Œª = quadgk(x ->fun(x)/(x-c), a, b)[1]
    end
    return Œª
end


function ratio(z, p; C = nothing)
    return G_int(z, p, C = C)/z
end


function find_ratio(z, p; iteration = 5)
    @unpack œÅ = p
    C0 = œÅ/z
    #G(k, C) = fÃÇ(k, p)/(1 - C * fÃÇ(k, p))
    C = C0
    for i = 1:iteration
        C = C0 / (1 -ratio(z, p, C = C))
    end
    return ratio(z, p, C = C)
end

function ratio_complex(Œª, p)
    @unpack N, œÅ = p
    _, a, b = gaussian_variation_model_fast_ab(Œª, vec(ones(N,1)), p, 1, 1)
    a, b = a/œÅ, b/œÅ
    ratio_re = 1 - a/Œª/(a^2+b^2)
    ratio_im = b/Œª/(a^2+b^2)
    return ratio_re, ratio_im
end

function ratio_theory(z, p)
    @unpack œµ, Œº, œÅ, n, L = p
    G_int = (œÄ/œµ)^Œº/Œº - œÅ^(Œº/(n-Œº))*z^(-Œº/(n-Œº))*œÄ*cot(œÄ*n/(n-Œº))*1.5 - (œÄ/L)^Œº/Œº
    return G_int/z
end

function generate_fÃÇ_interpolation(p; fun = fÃÇ_numerical, len = 10^4)
    @unpack œµ, L = p
    k‚ÇÅ = 10^-12#œÄ/L
    k‚ÇÇ = œÄ/œµ * 10
    k = range(k‚ÇÅ, k‚ÇÇ, length = len)
    f(x) = fun(x,p)
    fÃÇ_k = f.(k)
    return CubicSpline(fÃÇ_k,k)
end

function mds_fig(X, Corr, root)
    D2 = pairwise(Euclidean(), X)
    C2 = corr(D2,p) 
    Œª_sim, p_sim = eigendensity(C2, correction = false, Œª_min = 0.5)
    Œª_id = findall(Œª_sim.> 0.1 )
    plot(Œª_sim[Œª_id], p_sim[Œª_id], label = "$ifish _refactoring")
    plot!(xlabel = L"\lambda", ylabel = "pdf", xaxis = :log, yaxis = :log)
    Œª_sim, p_sim = eigendensity(Corr, correction = false, Œª_min = 0.5)
    plot!(Œª_sim[1:end], p_sim[1:end], label = "$ifish")
    plot!(title=L"L = %$(round(p.L,digits=3)), \mu = %$(round(p.Œº ,digits=3)), \epsilon = %$(round(p.œµ,digits=3)), n =  %$(p.n), \beta = %$(round(p.Œ≤,digits=3)), E(\sigma^4) = %$(round(p.œÉÃÑ‚Å¥,digits=2))")
    savefig(joinpath(root,"corr/lambda_density.png"))
    
    subsampling(C2, joinpath(root,"corr"))
    subsampling(Corr, joinpath(root,"corr/data"))

    R = hclust(D,linkage = :average)
    heatmap(Corr[R.order, R.order],theme = :dark, clim = (0,1))
    savefig(joinpath(root,"corr/corr.png"))
    heatmap(C2[R.order, R.order],theme = :dark, clim = (0,1))
    savefig(joinpath(root,"corr/corr_refactoring.png"))

    scatter(X[1,R.order],X[2,R.order], marker_z = 1:size(X,2), markersize = 1,  color = :jet)
    savefig(joinpath(root,"corr/point_cloud.png"))

    marginalkde(X[1,R.order],X[2,R.order])
    savefig(joinpath(root,"corr/marginalkde.png"))
end


function m_mds(C, p, resultroot; epoch = 10000, Descent0 = 1000, dacay_time = 10)
    @unpack Œ≤, n, N = p
    D = find_D(C, p)
    D = D - Diagonal(D)
    if Œ≤!=0
        global X, stress = mdscale(D,n = n, criterion = "sammon")
    else
        global X = randn(n,N)
    end
    C = C|>gpu
    global X = X|>gpu
    ps = Flux.params(X) |>gpu
    opt = Descent(Descent0)

    for i = 1:dacay_time
        opt = Descent(Descent0*exp((-i+1)*0.5))
        @time @epochs epoch Flux.train!(loss_mds, ps, [(C,p)], opt, cb = Flux.throttle(() -> println(loss_mds(C,p)), 10))
        time_now = Dates.format(now(), "Y_mm_dd_HH_MM")
        global X_cpu = X|>cpu
        resultroot2 = mkdir(joinpath(resultroot,"$time_now"))
        mkdir(joinpath(resultroot,"$time_now/corr"))
        mkdir(joinpath(resultroot,"$time_now/corr/data"))
        f = open(joinpath(resultroot2,"loss.txt"),"w")
            write(f,"loss=$(loss_mds(C,p))")
        close(f)
        matwrite(joinpath(resultroot2, "mmds"*"_$ifish.mat"),
            Dict("X"=>X_cpu', "ifish"=>ifish, "xi"=>p.Œæ, "L"=>p.L, "rho"=>p.œÅ, 
            "epsilon"=>p.œµ, "mu"=>p.Œº, "beta"=>p.Œ≤, "n"=>p.n, "loss" =>loss_mds(C,p)))
        if isnan(X_cpu[1])
            break
        end
        mds_fig(X_cpu, C|>cpu, resultroot2)
    end
end

#=
function corr(D,p)  
    @unpack Œº, œµ, Œ≤ = p 
    C_orr = copy(D)
    #C_orr[D .<= œµ] .= 1
    #C_orr[D .> œµ] .= œµ^Œº*(D[D .> œµ]).^(-Œº).*exp.(-(D[D .> œµ] .- œµ)*Œ≤)
    C_orr = min.(œµ^Œº*(D).^(-Œº).*exp.(-(D .- œµ)*Œ≤),1)
    return C_orr
end
=#

function corr(D,p)  
    @unpack Œº, œµ, Œ≤ = p 
    C_orr = copy(D)
    #C_orr[D .<= œµ] .= 1
    #C_orr[D .> œµ] .= œµ^Œº*(D[D .> œµ]).^(-Œº).*exp.(-(D[D .> œµ] .- œµ)*Œ≤)
    C_orr = œµ^Œº*(D.^2 .+œµ.^2).^(-Œº/2)
    return C_orr
end

#=
function find_D(C,p)
    @unpack Œº, œµ, Œ≤ = p 
    fc(D,C) = œµ^Œº*(D).^(-Œº).*exp.(-(D.- œµ)*Œ≤) - C
    N = size(C,1)
    fcd = ZeroProblem(fc,œµ)
    #a = solve(fcd,c)
    #d = find_zero(fc,œµ)
    [D[i,j] = solve(fcd,C[i,j]) for i = 1:N for j = 1:N]
    return D
end
=#

function find_D(C,p)
    @unpack Œº, œµ, Œ≤, L = p 
    L = L
    D = copy(C)
    D .= œµ*sqrt.(abs.(C.^(-2/Œº) .- 1))
    D[D.>L] .= L*log.((D[D.>L] ./L)) .+L
    return D
end

function CCA_corr_dim(Corr; n_all = 2:10, ifish = 210106, figroot = nothing)
    corrs = []
    if figroot == nothing 
        #figroot = "/home/wenlab-user/wangzezhen/FISHRNN_TEST/results/CCA/$ifish"
    end
    corr_shuffle = zeros(length(n_all), 100)
    i = 0
    for n in n_all
        i = i+1
        d = n
        œÅ, Œº = Parameter_estimation(Corr, n = n)
        N = size(Corr)[1]
        L = (N/œÅ)^(1/n)
        p = ERMParameter(;N = N, L = L, œÅ = œÅ, n = n, œµ = 0.03125, Œº = Œº, Œæ = 10, œÉÃÑ¬≤ = 1, œÉÃÑ‚Å¥ = 1)
        C = Corr
        #C = max.(C,0.01)
        C = abs.(C)
        D = copy(C)
        D = find_D(C, p)
        D = D - Diagonal(D)
        D = (D+D')/2
        #C = min.(C,1); D = sqrt.(1 .-C)
        #X = classical_mds(D, n); X = Float64.(X)
        X, stress = mdscale(D, n = n, criterion = "sammon")
        ROI_MDS_CCA = fit(CCA,ROIxyz',X)
        corr_cca = correlations(ROI_MDS_CCA)
        if n == 1
            corrs = vcat(corrs, corr_cca'[1])
            corrs = vcat(corrs, 0)
        else
            corrs = vcat(corrs, corr_cca'[1:2])
        end
        for j = 1:100
            shuffle_index = randperm(size(X,2))
            ROI_MDS_CCA_shuffle = fit(CCA,ROIxyz',X[:,shuffle_index])
            corr_shuffle[i,j] = correlations(ROI_MDS_CCA_shuffle)[1]
        end
    end
    corrs = reshape(corrs,2,:)
    plot(n_all,corrs[1,:],ylims = (0,1),label = "CCA_corr1,ifish = $ifish")
    plot!(n_all,corrs[2,:],ylims = (0,1),label = "CCA_corr2,ifish = $ifish")
    plot!(xlabel = "mds_dimension", ylabel = "CCA_corr")
    if figroot != nothing
        savefig(joinpath(figroot,"CCAcorrdim.png"))
        savefig(joinpath(figroot,"CCAcorrdim.svg"))
    end
    return corrs, corr_shuffle
end


function subsample_fig6(C, fig; subfig = 1, is_pyfig = false, fontsize =5)
    if is_pyfig == false
        pyfig = fig.o
    else
        pyfig = fig
    end
    plt.rc("font",family="Arial")
    plt.rc("pdf", fonttype=42)
    ax1 = pyfig[:axes][subfig]

    N = size(C,1)
    neural_set = random_clusters(N)
    iterations = floor(log2(N))
    cmap = plt.get_cmap("winter")
    colorList = cmap(range(0, 1, length=4))
    ax1.set_yscale("log")
    ax1.set_xscale("log")
    leg2 = []
    eigenspectrums = []
    nranks = []
    i = 0
    for n in range(iterations, 7, step=-1)
        i+=1
        eigenvalues, errorbars, k = pca_cluster2(neural_set, C, Int(2^n))
        I = findall(eigenvalues .> 0)
        eigenvalues = eigenvalues[I]
        k = k[I]
        errorbars = errorbars[I]
        ax1.plot(k[1:end], eigenvalues[1:end], linewidth=0.2,marker="o",color=colorList[i,:],
        ms=0.3,label=@sprintf("N=%d",2^n))   
        Œª_err_l = eigenvalues - errorbars./2
        Œª_err_u = eigenvalues + errorbars./2
        ax1.fill_between(k[1:end],Œª_err_l,Œª_err_u,alpha=0.3,color = colorList[i,:])

        append!(eigenspectrums,eigenvalues[4:end])
        append!(nranks, k[4:end])
    end
    ax1.set_ylim([1e-1, 1e+2])
    ax1.set_xlim([0.1^(3.2), 1])
    c_diag = sort(diag(C), rev=true)
    ax1.scatter((1:N)/N, c_diag,marker="o",label="diagonal",color="gray",alpha=0.3,s=2)
    ax1.set_xlabel("rank/N")
    ax1.set_ylabel("eigenvalue "* L"\lambda")
    #ax1.legend()
    ax1.legend(loc="upper right", fontsize = fontsize)
    return pyfig
end

function subsample_fig7(C, fig; subfig = 1, is_pyfig = false, fontsize =5, CI = 0.67, dataset= "original")
    if is_pyfig == false
        pyfig = fig.o
    else
        pyfig = fig
    end
    plt.rc("font",family="Arial")
    ax1 = pyfig[:axes][subfig]

    N = size(C,1)
    neural_set = random_clusters(N)
    iterations = floor(log2(N))
    cmap = plt.get_cmap("winter")
    colorList = cmap(range(0, 1, length=4))
    ax1.set_yscale("log")
    ax1.set_xscale("log")
    leg2 = []
    eigenspectrums = []
    nranks = []
    i = 0
    for n in range(iterations, 7, step=-1)
        i+=1
        eigenvalues, errorbars, k = pca_cluster2(neural_set, C, Int(2^n))
        I = findall(eigenvalues .> 0)
        eigenvalues = eigenvalues[I]
        k = k[I]
        errorbars = errorbars[I]
        ax1.plot(k[1:end], eigenvalues[1:end], linewidth=0.2,marker="o",color=colorList[i,:],
        ms=0.3,label=@sprintf("N=%d",2^n))   
        Œª_err_l = eigenvalues - errorbars./2
        Œª_err_u = eigenvalues + errorbars./2
        ax1.fill_between(k[1:end],Œª_err_l,Œª_err_u,alpha=0.3,color = colorList[i,:])

        append!(eigenspectrums,eigenvalues[4:end])
        append!(nranks, k[4:end])
    end
    ax1.set_ylim([1e-1, 1e+2])
    ax1.set_xlim([0.1^(3.2), 1])
    c_diag = sort(diag(C), rev=true)
    ax1.scatter((1:N)/N, c_diag,marker="o",label="diagonal",color="gray",alpha=0.3,s=2)
    ax1.set_xlabel("rank/N")
    ax1.set_ylabel("eigenvalue "* L"\lambda")
    #ax1.legend()
    ax1.legend(loc="upper right", fontsize = fontsize)
    ax1.text(1e-3,0.3,s="CI = $CI", fontsize = 2*fontsize)
    ax1.text(1e-3,1,s=dataset, fontsize = 2*fontsize)
    return pyfig
end

function multi_sim_subplot(ax,p,n_sim;different_neural_variability=false)

    ax.set_yscale("log")
    ax.set_xscale("log")
    ax.set_xlabel(L"\lambda")
    ax.set_ylabel(L"p(\lambda)")
    # n_sim = 10
    neural_set = random_clusters(N)
    points = rand(Uniform(-p.L/2,p.L/2),p.N,p.n) #points are uniformly distributed in a region 
    C = reconstruct_covariance(points, p, subsample = false)
    eigenspectrum, k = pca_cluster2_multi_sim(neural_set, C, p.N)
    eigenspectrum_all = copy(eigenspectrum)
    Threads.@threads for ii = 2:n_sim
        points = rand(Uniform(-p.L/2,p.L/2),p.N,p.n) #points are uniformly distributed in a region 
        C = reconstruct_covariance(points, p, subsample = false)
        eigenspectrum, k = pca_cluster2_multi_sim(neural_set, C, p.N)
        eigenspectrum_all = hcat(eigenspectrum_all,eigenspectrum)
    end
    eigenvalues = mean(eigenspectrum_all, dims=2)
    errorbars = std(eigenspectrum_all, dims=2) ./ sqrt(size(eigenspectrum_all,2))

    Œª_min = minimum(eigenspectrum[:])
    Œª_max = maximum(eigenspectrum[:])

    id, Œª = dropzeros_and_less(eigenspectrum_all[:,1])
    length = 200
    h_Œª = fit(Histogram, Œª, LogRange(Œª_min,Œª_max,length))
    # edges, p_sim = normalize(h_Œª, mode=:pdf)
    h_Œª = normalize(h_Œª, mode=:pdf)
    @unpack edges, weights = h_Œª       
    binsize = edges[1][2] - edges[1][1]
    x = collect(edges[1][2:end].-binsize/2)
    p_sim_all = vec(weights.*size(id,1)/p.N)

    for ii = 2:size(eigenspectrum_all,2)
        id, Œª = dropzeros_and_less(eigenspectrum_all[:,ii])
        length = 200
        h_Œª = fit(Histogram, Œª, LogRange((Œª_min),maximum(Œª_max),length))
        h_Œª = normalize(h_Œª, mode=:pdf)
        @unpack edges, weights = h_Œª   
        binsize = edges[1][2] - edges[1][1]
        x = collect(edges[1][2:end].-binsize/2)
        p_sim = vec(weights.*size(id,1)/p.N)
        p_sim_all = hcat(p_sim_all,p_sim)
    end

    p_sim,errorbars = mean_sem_nonzero(p_sim_all, dims=2)
    # errorbars = std(p_sim_all, dims=2) ./ sqrt(size(p_sim_all,2))
    p_err_l = p_sim .- errorbars
    p_err_u = p_sim .+ errorbars

    ax.plot(x, p_sim, linewidth=1,label="ERM simulation")        
    ax.fill_between(x,p_err_l[1:end],p_err_u[1:end],alpha=0.3)


    return ax
end

function subsample_fig4(C, p, fig; subfig = 1, is_pyfig = false, fontsize =5, fontsize2=10,n_sim = 100)
    if is_pyfig == false
        pyfig = fig.o
    else
        pyfig = fig
    end
    plt.rc("font",family="Arial")
    plt.rc("pdf", fonttype=42)
    ax = pyfig[:axes][subfig]

    Œª_sim, p_sim, Œª_theory, p_theory = eigendensity(C, p, correction = false, Œª_min = 0.8, Œª_max = 50);
    _, _, Œª_theory_GV, p_theory_GV = eigendensity(C, p; length = 20, use_variation_model = true, Œª_min = 0.8, Œª_max = 50);
    multi_sim_subplot(ax,p,n_sim)
    #plt.rc("font",family="Arial")
    ax.plot(Œª_theory, p_theory, label = "high density theory",lw=1,color="gray")
    ax.plot(Œª_theory_GV[1:end], p_theory_GV[1:end],lw=1, label = "variational method",color="red")
    ax.loglog()
    # ax.grid(b=true)
    ax.set_xlabel("eigenvalue "* L"\lambda",fontsize = fontsize2)
    ax.set_ylabel("probability density " *L"p(\lambda)",fontsize = fontsize2)
    # ax.legend(loc="bottom right",bbox_to_anchor=(0.5, -0.15))
    ax.legend(loc="lower left", fontsize = fontsize)
    # plt.savefig("high_vs_variation.pdf",bbox_inches="tight",dpi=300)
    ax.set_xlim((1e-2,1e3/2))
    ax.set_ylim(1e-5,1e1)
    #ax.text(-0.1, 1.05, uppercase_letters[ipanel],transform=ax.transAxes,size=10,weight="bold")

    return pyfig
end

function corr2cov(Corr, C·µ£)
    std_c = sqrt(Diagonal(C·µ£))
    Cov = std_c*Corr*std_c
    Cov = (Cov+Cov')/2
end


function linearize_triu(A)
    # V is the upper triangular matrix of A in the vector form, excluding the main diagonal
    N = size(A, 1)
    L = Int(N * (N - 1) / 2)
    V = zeros(L)
    IDX2SUB = zeros(L, 2)
    count = 1
    for i in 1:N
        for j in i+1:N
            V[count] = A[i, j]
            IDX2SUB[count, :] = [i, j]
            count += 1
        end
    end
    return V, IDX2SUB
end


function normalize_activity(a)
    N = size(a,1)
    epsilon = 1e-4
    a[a.<epsilon] .= 0
    for i = 1:N
        IDX = a[i,:] .> 0
        mean_activity = mean(a[i,IDX]);
        if mean_activity > 0
            a[i,:] = a[i,:]./mean_activity
        end
    end
    return a
end

function data_preprocessing(FR)

    tf = vec(sum(FR, dims=2) .> 0)
    FR = convert(Array{Float64}, FR[tf, :])
    FR = normalize_activity(FR);

    K = 1024
    # Random.seed!(1)
    i_rand = randperm(size(FR, 1))[1:K]
    FR_sub = FR[i_rand,:]
    C = cov(convert(Matrix, FR_sub'))
    s = diag(C)
    s = mean(s)
    C = C ./ s

    return FR_sub, C
end


function A2corr(a)
    #a = a .- mean(a,dims=2)
    (N,T)=size(a)
    C·µ£ = a*a'/T 
    Œ≤ = N/tr(C·µ£)
    C·µ£ = C·µ£*Œ≤

    global id_un0 = findall(diag(C·µ£) .!=0) 
    C·µ£ =  C·µ£[id_un0,id_un0]

    std_c = sqrt(Diagonal(C·µ£))
    Corr = std_c\C·µ£/std_c
    Corr = (Corr + Corr')/2
    return C·µ£, Corr, id_un0
end

function cov_behavior(firingrate, y_conv_range)
    frame_behavior = findall(y_conv_range[:,1] .==1)
    frame_nobehavior = findall(y_conv_range[:,1] .==0)
    frame_all_1 = sample(frame_nobehavior, length(frame_nobehavior)-length(frame_behavior), replace = false)
    frame_all_2 = vcat(frame_all_1, frame_behavior)

    
    r_behavior = firingrate[:, frame_all_2]
    r_nobehavior = firingrate[:, frame_nobehavior]

    A_nobehavior = normalization(r_nobehavior, œµ=1e-4)
    A_nobehavior = A_nobehavior .- mean(A_nobehavior,dims=2)

    A_behavior = normalization(r_behavior, œµ=1e-4)
    A_behavior = A_behavior .- mean(A_behavior,dims=2)

    Cov_nobehavior, Corr_nobehavior, id_un0_nobehavior = A2corr(A_nobehavior)
    Cov_behavior, Corr_behavior, id_un0_behavior = A2corr(A_behavior)
    return Cov_behavior, Cov_nobehavior
end

function corr_behavior(firingrate, y_conv_range)
    frame_behavior = findall(y_conv_range[:,1] .==1)
    frame_nobehavior = findall(y_conv_range[:,1] .==0)
    frame_all_1 = sample(frame_nobehavior, length(frame_nobehavior)-length(frame_behavior), replace = false)
    frame_all_2 = vcat(frame_all_1, frame_behavior)

    
    r_behavior = firingrate[:, frame_all_2]
    r_nobehavior = firingrate[:, frame_nobehavior]

    A_nobehavior = normalization(r_nobehavior, œµ=1e-4)
    A_nobehavior = A_nobehavior .- mean(A_nobehavior,dims=2)

    A_behavior = normalization(r_behavior, œµ=1e-4)
    A_behavior = A_behavior .- mean(A_behavior,dims=2)

    Cov_nobehavior, Corr_nobehavior, id_un0_nobehavior = A2corr(A_nobehavior)
    Cov_behavior, Corr_behavior, id_un0_behavior = A2corr(A_behavior)
    return Corr_behavior, Corr_nobehavior
end


function quantile_variational(Œª, p; length_range = 20, œÇ¬≤ = nothing ,p0 = 0.01, Œª_max = nothing)
    @unpack œÅ, Œº, œµ, n, Œ≤, L= p
    N = œÅ*L^n
    if œÇ¬≤ == nothing
        œÇ¬≤ = vec(ones(round(Int,N)))
    end
    t = 1#round(N/20)
    if Œª_max == nothing #|| Œª_max != nothing
        p0 = t/N
        #t = p0*N
        Œª_max = œÅ*fÃÇ(2*sqrt(œÄ)/L*sqrt(t),p)
    end
    #Œª_all = range(Œª, Œª_max ,length = length_range)
    Œª_all = LogRange(Œª, Œª_max, length_range)
    dŒª = Œª_all[2] - Œª_all[1]
    dlnŒª = log(Œª_all[2]/Œª_all[1])
    GVMF(Œª_all) = gaussian_variation_model_fast(Œª_all, œÇ¬≤, p, 1, 1)
    p_all = GVMF.(Œª_all)
    #q = sum(p_all) * dŒª
    q = (sum(p_all.*Œª_all) - p_all[1].*Œª_all[1]/2 - p_all[end].*Œª_all[end]/2)*length_range/(length_range-1) * dlnŒª + p0
    return q
end

function dqdœÅ(Œª; p = nothing, dœÅ = 0.01)
    if p == nothing
        N = 1024; L = 10; œÅ = 5.12; n = 2; œµ = 0.03125; Œº = 0.5; Œæ = 10
    else
        @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    end
    p0 = ERMParameter(;N = N, L = L, œÅ = œÅ, n = n, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ =Œ≤, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    q_Œª = quantile_variational(Œª, p0)
    p1 = ERMParameter(;N = N, L = L, œÅ = œÅ+dœÅ, n = n, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ =Œ≤, œÉÃÑ¬≤ = mean(œÇ¬≤), œÉÃÑ‚Å¥ = mean(œÇ¬≤.^2))
    q_Œª1 = quantile_variational(Œª, p1)
    dq_ŒªdœÅ = (q_Œª1 - q_Œª)/dœÅ
    return dq_ŒªdœÅ
end

function dlnqdlnœÅ(Œª;œÇ¬≤ =nothing, length_range = 20, p = nothing, dœÅ = 0.01)
    if p == nothing
        N = 1024; L = 10; œÅ = 5.12; n = 2; œµ = 0.03125; Œº = 0.5; Œæ = 10
    else
        @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    end
    p0 = ERMParameter(;N = N, L = L, œÅ = œÅ, n = n, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ =Œ≤, œÉÃÑ¬≤ = 1, œÉÃÑ‚Å¥ = 1)
    q_Œª = quantile_variational(Œª, p0, length_range = length_range,œÇ¬≤ = œÇ¬≤)
    p1 = ERMParameter(;N = N, L = L, œÅ = œÅ+dœÅ, n = n, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ =Œ≤, œÉÃÑ¬≤ = 1, œÉÃÑ‚Å¥ = 1)
    q_Œª1 = quantile_variational(Œª, p1, length_range = length_range,œÇ¬≤ = œÇ¬≤)
    dlnq_ŒªdlnœÅ = (q_Œª1 - q_Œª)/dœÅ /q_Œª*œÅ
    return dlnq_ŒªdlnœÅ
end
   
function dlnqdlnœÅ2(Œª;œÇ¬≤ =nothing, length_range = 20, p = nothing, p2 =nothing, dœÅ = 0.01,Œª_max01=nothing,Œª_max02=nothing,p01=nothing,p02=nothing)
    if p == nothing
        N = 1024; L = 10; œÅ = 5.12; n = 2; œµ = 0.03125; Œº = 0.5; Œæ = 10
    else
        @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    end
    q_Œª = quantile_variational(Œª, p, length_range = length_range, œÇ¬≤ = œÇ¬≤, Œª_max=Œª_max01, p0=p01)
    q_Œª1 = quantile_variational(Œª, p2, length_range = length_range, œÇ¬≤ = œÇ¬≤, Œª_max=Œª_max02, p0=p02)
    dlnq_ŒªdlnœÅ = (log(q_Œª1) -log(q_Œª))/log(p2.œÅ/p.œÅ)
    return dlnq_ŒªdlnœÅ
end

function dlnqdlnœÅ3(Œª;œÇ¬≤ =nothing, length_range = 20, p = nothing, p2 =nothing, p3 =nothing, dœÅ = 0.01,Œª_max01=nothing,Œª_max02=nothing,Œª_max03=nothing,p01=nothing,p02=nothing,p03=nothing)
    if p == nothing
        N = 1024; L = 10; œÅ = 5.12; n = 2; œµ = 0.03125; Œº = 0.5; Œæ = 10
    else
        @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    end
    q_Œª = quantile_variational(Œª, p, length_range = length_range, œÇ¬≤ = œÇ¬≤, Œª_max=Œª_max01, p0=p01)
    q_Œª1 = quantile_variational(Œª, p2, length_range = length_range, œÇ¬≤ = œÇ¬≤, Œª_max=Œª_max02, p0=p02)
    q_Œª3 = quantile_variational(Œª, p3, length_range = length_range, œÇ¬≤ = œÇ¬≤, Œª_max=Œª_max03, p0=p03)
    dlnq_ŒªdlnœÅ = (q_Œª1 -q_Œª)/q_Œª3/log(p2.œÅ/p.œÅ)
    return dlnq_ŒªdlnœÅ
end

function findmax_Œª(p::ERMParameter, œÇ¬≤; p0 = 0.01, ntrial = 100)
    s = 0.5 # subsampling fraction
    @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    id_c = sample(1:N, N, replace =false)
    lb_N = zeros(N,ntrial)
    Threads.@threads for i in 1:ntrial
        points1 = rand(Uniform(-L/2,L/2),N,n)
        œÉ = vec(broadcast(‚àö,œÇ¬≤))[id_c]
        Œî = diagm(œÉ)
        Œî = Œî[id_c,id_c]
        C = reconstruct_covariance(points1, p, subsample = false)
        lb_N[:,i] = -sort(-eigvals(Hermitian(C)))
    end

    lb_N = vec(mean(lb_N,dims=2))
    r0 = round(Int, N*p0-0.5)
    dr0 = Int(1)
    dr = dr0+1 #Int.(sign(r0%2-0.5)+dr0)
    #return sqrt(lb_N[r0+dr] * lb_N[r0+dr0]), sqrt((r0+dr)/N * (r0+dr0)/N)
    return exp(mean([log(lb_N[r0+dr]), log(lb_N[r0+dr0]), log(lb_N[r0+dr0-1]) ])), exp(mean([log((r0+dr)/N), log((r0+dr0)/N), log((r0+dr0-1)/N) ]))
    #return lb_N[r0+1], (r0+1)/N
end

function findmax_Œª0(p::ERMParameter, œÇ¬≤; p0 = 0.01, ntrial = 100)
    s = 0.5 # subsampling fraction
    @unpack N, L, œÅ, n, œµ, Œº, Œæ, Œ≤ = p
    id_c = sample(1:N, N, replace =false)
    lb_N = zeros(N,ntrial)
    Threads.@threads for i in 1:ntrial
        points1 = rand(Uniform(-L/2,L/2),N,n)
        œÉ = vec(broadcast(‚àö,œÇ¬≤))[id_c]
        Œî = diagm(œÉ)
        Œî = Œî[id_c,id_c]
        C = reconstruct_covariance(points1, p, subsample = false)
        lb_N[:,i] = -sort(-eigvals(Hermitian(C)))
    end
    lb_N = vec(mean(lb_N,dims=2))
    r0 = round(Int, N*p0-0.5)
    r1 = maximum(findall(lb_N .> 1))
    dr0 = Int(1)
    return lb_N[r0+1], (r0+1)/N, lb_N[r1], r1/N
end

function findmax_Œª(C; p0 = 0.01)
    s = 0.5 # subsampling fraction

    N,_ = size(C)
    Ns = round(Int, N*s)
    lb_N = -sort(-eigvals(Hermitian(C)))
    ntrial = 20
    lb_Ns = zeros(Ns,ntrial)
    Threads.@threads for i in 1:ntrial
        i_s = randperm(N)[1:Ns]
        Cs = copy(C[:,i_s])
        Cs = Cs[i_s,:]
        lb_Ns[:,i] = -sort(-eigvals(Hermitian(Cs)))
    end
    lb_Ns = vec(mean(lb_Ns,dims=2))

    r0 = round(Int, Ns*p0-0.5) # lambda_max cutoff
    r1 = maximum(findall(lb_Ns .> 1)) # lambda_min cutoff
    f_Ns = log.(lb_Ns[r0+1:r1])
    f_N = zeros(length(f_Ns))
    return lb_Ns[r0]
end


function collapse_index_rank_theory(p, C; p0 = 0.01,œÇ¬≤ = nothing, ntrial = 1, Œª_min=1, len =100, dŒª =0.01, length_range = 20)
    @unpack œÅ, Œº, œµ, n, Œ≤, N, Œæ = p
    œÇ¬≤ = diag(C)

    p2 = ERMParameter(;N = N/2, L = L, œÅ = œÅ/2, n = d, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ = 0, œÉÃÑ¬≤ = 1, œÉÃÑ‚Å¥ = 1)
    p3 = ERMParameter(;N = round(Int,N/sqrt(2)), L = L, œÅ = œÅ/sqrt(2), n = d, œµ = œµ, Œº = Œº, Œæ = Œæ, Œ≤ = 0, œÉÃÑ¬≤ = 1, œÉÃÑ‚Å¥ = 1)

    #Œª_max, p00 = findmax_Œª0(p2, œÇ¬≤, p0 =p0)
    Œª_max, p00, Œª_min, p0min = findmax_Œª0(p2, œÇ¬≤, p0 =p0)
    #Œª_max, p00 = findmax_Œª(C, p0 =p0)
    Œª_all =  Œª_min:(Œª_max - Œª_min) / (len - 1):Œª_max

    pt = 0.01
    Œª_max01, p01 = findmax_Œª(p, œÇ¬≤, p0 =pt)
    Œª_max02, p02 = findmax_Œª(p2, œÇ¬≤, p0 =pt)
    Œª_max03, p03 = findmax_Œª(p3, œÇ¬≤, p0 =pt)

    p_L = gaussian_variation_model_fast(œÅ*fÃÇ(œÄ/L,p) , œÇ¬≤, p, 1, 1)
    GVMF(Œª_all) = gaussian_variation_model_fast(Œª_all, œÇ¬≤, p, 1, 1)
    #p_all = GVMF.(Œª_all)
    #dlnqdlnœÅ_all = dlnqdlnœÅ.(Œª_all,p=p3,œÇ¬≤=œÇ¬≤, length_range = length_range)
    dlnqdlnœÅ_all = dlnqdlnœÅ2.(Œª_all,p=p,p2=p2,œÇ¬≤=œÇ¬≤,length_range=length_range,Œª_max01=Œª_max01,Œª_max02=Œª_max02,p01=p01,p02=p02)
    #dlnqdlnœÅ_all = dlnqdlnœÅ3.(Œª_all,p=p,p2=p2,p3=p3,œÇ¬≤=œÇ¬≤,length_range=length_range,Œª_max01=Œª_max01,Œª_max02=Œª_max02,Œª_max03=Œª_max03,p01=p01,p02=p02,p03=p03)

    A = hcat(Œª_all,dlnqdlnœÅ_all)
    itp = Interpolations.scale(interpolate(A, (BSpline(Cubic(Natural(OnGrid()))), NoInterp())), Œª_all, 1:2)

    tfine = Œª_min:dŒª:Œª_max
    Œª2, dlnqdlnœÅ_all2 = [itp(t,1) for t in tfine], [itp(t,2) for t in tfine]

    #index = mean((abs.(dlnqdlnœÅ_all2))./Œª2) *(Œª_max-Œª_min)/abs(log.(p00)-log.(quantile_variational(Œª_min,p2,œÇ¬≤=œÇ¬≤, Œª_max=Œª_max02, p0=p02)))
    index = mean((abs.(dlnqdlnœÅ_all2))./Œª2) *(Œª_max-Œª_min)/abs(log.(quantile_variational(Œª_max,p2,œÇ¬≤=œÇ¬≤, Œª_max=Œª_max02, p0=p02))-log.(quantile_variational(Œª_min,p2,œÇ¬≤=œÇ¬≤, Œª_max=Œª_max02, p0=p02)))
    #index = mean((abs.(dlnqdlnœÅ_all2))./Œª2) *(Œª_max-Œª_min)/abs(log.(p03)-log.(quantile_variational(Œª_min,p3,œÇ¬≤=œÇ¬≤, Œª_max=Œª_max03, p0=p03)))
    #index = mean((abs.(dlnqdlnœÅ_all2))./Œª2) *(Œª_max-Œª_min)/abs(log.(p00)-log.(p0min))
    return index
end


function arrow_color(X01, k01, X_pjec; color_map = :PRGn, num = 1000)
    X_all = zeros(2,num)
    for i = 1:num
        X_all[1,i] = X01[1] + k01*X_pjec[1,1] *(i-1)/(num-1)
        X_all[2,i] = X01[2] + k01*X_pjec[2,1] *(i-1)/(num-1)
    end
    for i = 1:num-50
        plot!([X_all[1,i],X_all[1,i+1]],[X_all[2,i],X_all[2,i+1]],arrow=false,color=color_map[i],linewidth=4,label="",legend=:topleft)
    end
    plot!([X_all[1,num-1],X_all[1,num]],[X_all[2,num-1],X_all[2,num]],arrow=Plots.arrow(:closed, 1.0),color=color_map[num],linewidth=1,label="",legend=:topleft)
    #return plot!([X_all[1,num-1],X_all[1,num]],[X_all[2,num-1],X_all[2,num]],arrow=true,color_map=color_map[num],linewidth=2,label="",legend=:topleft)
    return plot!()
end

#=
function arrow_color(X01, k01; color_map = :PRGn,  num = 100)
    X_all = zeros(num-1,2)
    Y_all = zeros(num-1,2)
    for i = 1:num-1
        X_all[i,1] = X01[1] + k01*X_pjec[1,1] *(i-1)/(num-1)
        X_all[i,2] = X01[1] + k01*X_pjec[1,1] *(i)/(num-1)
        Y_all[i,1] = X01[2] + k01*X_pjec[2,1] *(i-1)/(num-1)
        Y_all[i,2] = X01[2] + k01*X_pjec[2,1] *(i)/(num-1)
    end
    plot!(X_all,Y_all,line_z=(1:num-1)/num',arrow=false,color=color_map,linewidth=2,label="",legend=:topleft)#, colorbar=false)
    #plot(X_all,Y_all,line_z=(1:num-1)/100',arrow=false,color=palette(cc),linewidth=2,label="",legend=:topleft, colorbar=false)
    return plot!(X_all[end,:],Y_all[end,:],line_z=1,arrow=true,color=color_map,linewidth=2,label="",legend=:topleft)#, colorbar=false)
end
=#


function palette_colormap(color_map; N_color =1000)
    p_c = palette(color_map)
    l = size(p_c)[1]
    c_mp = colormap("RdBu", N_color)
    for i = 1:N_color
        j0 = i/N_color*(l-1)+1
        j = floor(Int,i/N_color*(l-1)+1-1/N_color/10)
        c_mp[i] = (j+1-j0) * p_c[j] + (j0-j) * p_c[j+1]
    end
    return c_mp
end


function plot_scale_fig6(;dpi=300, fsize = 6,yscale = 300)
    plot!([340,390],[yscale,yscale],color = :black,grid = :none,label="",legend=:none, dpi = dpi)
    #plot!([340,340],[235,260],color = :black,grid = :none,label="",legend=:none, dpi = dpi)
    annotate!(365,yscale-13, Plots.text("100 Œºm",fsize), dpi = dpi) 
    #annotate!(315,247.5, Plots.text(L"50\mu m",10), dpi = dpi)
end